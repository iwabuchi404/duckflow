"""
PromptSmith ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹è§£æã®çµ±åˆãƒ†ã‚¹ãƒˆ
ã‚·ãƒŠãƒªã‚ªç”Ÿæˆã€å®Ÿè¡Œã€è©•ä¾¡ã®å…¨ä½“çš„ãªçµ±åˆãƒ†ã‚¹ãƒˆ
"""

import pytest
import tempfile
import json
from pathlib import Path
from datetime import datetime

from codecrafter.promptsmith.ai_roles.file_analysis_scenarios import (
    FileAnalysisScenarioGenerator, 
    FileAnalysisScenario
)
from codecrafter.promptsmith.evaluation.file_analysis_evaluator import (
    FileAnalysisEvaluator,
    AnalysisEvaluationResult
)

class TestFileAnalysisIntegration:
    """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®åˆæœŸåŒ–"""
        self.generator = FileAnalysisScenarioGenerator()
        self.evaluator = FileAnalysisEvaluator()
    
    def test_scenario_generation(self):
        """ã‚·ãƒŠãƒªã‚ªç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
        
        # å„ãƒ¬ãƒ™ãƒ«ã®ã‚·ãƒŠãƒªã‚ªç”Ÿæˆ
        for level in [1, 2, 3]:
            scenario = self.generator.generate_scenario(level=level)
            
            assert isinstance(scenario, FileAnalysisScenario)
            assert scenario.level == level
            assert scenario.scenario_id is not None
            assert len(scenario.user_request) > 0
            assert len(scenario.expected_analysis_points) > 0
            assert len(scenario.evaluation_criteria) > 0
            assert Path(scenario.file_path).exists()
    
    def test_batch_scenario_generation(self):
        """ãƒãƒƒãƒã‚·ãƒŠãƒªã‚ªç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
        
        batch = self.generator.create_scenario_batch(batch_size=6)
        
        assert batch["total_scenarios"] == 6
        assert len(batch["scenarios"]) == 6
        assert "batch_id" in batch
        assert "level_distribution" in batch
        
        # ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒç¢ºèª
        levels = [s.level for s in batch["scenarios"]]
        assert 1 in levels
        assert 2 in levels
        assert 3 in levels
    
    def test_evaluation_system(self):
        """è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚·ãƒŠãƒªã‚ª
        scenario = self.generator.generate_scenario(level=2, category="bug_detection")
        
        # ã‚µãƒ³ãƒ—ãƒ«å›ç­”
        test_response = """
        ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’åˆ†æã—ãŸçµæœã€ä»¥ä¸‹ã®å•é¡Œã‚’ç™ºè¦‹ã—ã¾ã—ãŸï¼š
        
        1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ¬ å¦‚
           - JSON.parseã§ä¾‹å¤–å‡¦ç†ãŒã•ã‚Œã¦ã„ã¾ã›ã‚“
           - try-catchæ–‡ã®è¿½åŠ ãŒå¿…è¦ã§ã™
        
        2. null/undefinedãƒã‚§ãƒƒã‚¯ä¸è¶³
           - userã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®nullãƒã‚§ãƒƒã‚¯ãŒä¸ååˆ†ã§ã™
           
        3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®å•é¡Œ
           - SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è„†å¼±æ€§ãŒå­˜åœ¨ã—ã¾ã™
        
        ä¿®æ­£ææ¡ˆï¼š
        - é©åˆ‡ãªä¾‹å¤–å‡¦ç†ã®å®Ÿè£…
        - å…¥åŠ›å€¤æ¤œè¨¼ã®å¼·åŒ–
        - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªã®ä½¿ç”¨
        """
        
        # è©•ä¾¡å®Ÿè¡Œ
        evaluation = self.evaluator.evaluate_analysis(scenario, test_response)
        
        assert isinstance(evaluation, AnalysisEvaluationResult)
        assert evaluation.scenario_id == scenario.scenario_id
        assert 0 <= evaluation.total_score <= 1
        assert len(evaluation.category_scores) > 0
        assert len(evaluation.detailed_feedback) > 0
    
    def test_low_quality_response_evaluation(self):
        """ä½å“è³ªå›ç­”ã®è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
        
        scenario = self.generator.generate_scenario(level=1)
        
        # ä½å“è³ªãªå›ç­”
        poor_response = "ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½•ã‹ã‚’ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚"
        
        evaluation = self.evaluator.evaluate_analysis(scenario, poor_response)
        
        # ä½å“è³ªå›ç­”ã¯ä½ã‚¹ã‚³ã‚¢ã‚’å—ã‘ã‚‹ã¹ã
        assert evaluation.total_score < 0.5
        assert len(evaluation.improvement_suggestions) > 0
        assert len(evaluation.weaknesses) > 0
    
    def test_high_quality_response_evaluation(self):
        """é«˜å“è³ªå›ç­”ã®è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
        
        scenario = self.generator.generate_scenario(level=2)
        
        # é«˜å“è³ªãªå›ç­”
        high_quality_response = """
        ## ã‚³ãƒ¼ãƒ‰æ§‹é€ åˆ†æ
        
        ã“ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š
        
        ### ã‚¯ãƒ©ã‚¹å®šç¾©
        1. **DataProcessor**ï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ï¼‰
           - æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹ (ABC)
           - process()æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å¥‘ç´„ã‚’å®šç¾©
           - validate_data(): ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼ï¼ˆDictå‹ã®å¿…é ˆã‚­ãƒ¼ç¢ºèªï¼‰
           
        2. **TextProcessor**ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ï¼‰
           - DataProcessorã‚’ç¶™æ‰¿ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†å®Ÿè£…
           - ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿å¼•æ•°: name(str), max_length(int, default=1000)
           - process()ãƒ¡ã‚½ãƒƒãƒ‰: ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å˜èªæ•°ã‚«ã‚¦ãƒ³ãƒˆ
        
        ### ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰ã®è©³ç´°åˆ†æ
        
        **process()ãƒ¡ã‚½ãƒƒãƒ‰**:
        - å¼•æ•°: data (Dict) - id, content, timestampã‚­ãƒ¼ãŒå¿…é ˆ
        - æˆ»ã‚Šå€¤: Dict - å‡¦ç†çµæœï¼ˆprocessed_content, word_countç­‰ï¼‰
        - å‡¦ç†ãƒ•ãƒ­ãƒ¼: æ¤œè¨¼ â†’ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ çµ±è¨ˆè¨ˆç®— â†’ çµæœè¿”å´
        
        **_clean_text()ãƒ¡ã‚½ãƒƒãƒ‰**:
        - ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ï¼šãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–å‡¦ç†
        - ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»æ©Ÿèƒ½
        - å°æ–‡å­—å¤‰æ›ã¨ãƒˆãƒªãƒ å‡¦ç†
        
        ### è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ç‰¹å¾´
        - **Template Method Pattern**: åŸºåº•ã‚¯ãƒ©ã‚¹ã§å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©
        - **Strategy Patternçš„ãªç¶™æ‰¿**: å‡¦ç†ç¨®é¡åˆ¥ã®å®Ÿè£…åˆ†é›¢
        - **å‹å®‰å…¨æ€§**: type hintsã«ã‚ˆã‚‹å¼•æ•°ãƒ»æˆ»ã‚Šå€¤ã®å‹ä¿è¨¼
        
        ### å“è³ªè©•ä¾¡
        **å¼·ã¿:**
        - æŠ½è±¡åŒ–è¨­è¨ˆã«ã‚ˆã‚‹æ‹¡å¼µæ€§
        - é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆValueErrorï¼‰
        - ãƒ­ã‚°å‡ºåŠ›ã«ã‚ˆã‚‹å®Ÿè¡Œå¯è¦–æ€§
        
        **æ”¹å–„ææ¡ˆ:**
        - ã‚ˆã‚Šè©³ç´°ãªä¾‹å¤–ã‚¿ã‚¤ãƒ—ã®å®šç¾©
        - ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
        - è¨­å®šã®å¤–éƒ¨åŒ–ï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆç­‰ï¼‰
        """
        
        evaluation = self.evaluator.evaluate_analysis(scenario, high_quality_response)
        
        # é«˜å“è³ªå›ç­”ã¯é«˜ã‚¹ã‚³ã‚¢ã‚’å—ã‘ã‚‹ã¹ã
        assert evaluation.total_score > 0.7
        assert len(evaluation.strengths) > len(evaluation.weaknesses)
    
    def test_batch_evaluation(self):
        """ãƒãƒƒãƒè©•ä¾¡ã®ãƒ†ã‚¹ãƒˆ"""
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚·ãƒŠãƒªã‚ªã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æº–å‚™
        scenarios = []
        responses = []
        
        for level in [1, 2]:
            scenario = self.generator.generate_scenario(level=level)
            scenarios.append(scenario)
            
            # ãƒ¬ãƒ™ãƒ«åˆ¥ã®ã‚µãƒ³ãƒ—ãƒ«å›ç­”
            if level == 1:
                response = "ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ calculate_area é–¢æ•°ãŒã‚ã‚Šã€å¹…ã¨é«˜ã•ã‚’å¼•æ•°ã¨ã—ã¦é¢ç©ã‚’è¿”ã—ã¾ã™ã€‚"
            else:
                response = """
                ã‚³ãƒ¼ãƒ‰åˆ†æçµæœ:
                1. ã‚¯ãƒ©ã‚¹æ§‹é€ : ç¶™æ‰¿é–¢ä¿‚ã®ã‚ã‚‹è¤‡é›‘ãªè¨­è¨ˆ
                2. å•é¡Œç‚¹: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³
                3. æ”¹å–„æ¡ˆ: ä¾‹å¤–å‡¦ç†ã®è¿½åŠ ã¨ãƒ­ã‚°æ”¹å–„
                """
            
            responses.append(response)
        
        scenarios_and_responses = list(zip(scenarios, responses))
        
        # ãƒãƒƒãƒè©•ä¾¡å®Ÿè¡Œ
        batch_result = self.evaluator.batch_evaluate(scenarios_and_responses)
        
        assert "batch_id" in batch_result
        assert batch_result["statistics"]["total_evaluations"] == 2
        assert "average_score" in batch_result["statistics"]
        assert "score_distribution" in batch_result["statistics"]
        assert len(batch_result["individual_results"]) == 2
    
    def test_challenge_scenario_generation(self):
        """æŒ‘æˆ¦çš„ã‚·ãƒŠãƒªã‚ªç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
        
        challenge_scenarios = self.generator.generate_challenge_scenarios(count=3)
        
        assert len(challenge_scenarios) == 3
        
        # å„ã‚·ãƒŠãƒªã‚ªã®åŸºæœ¬æ¤œè¨¼
        for scenario in challenge_scenarios:
            assert isinstance(scenario, FileAnalysisScenario)
            assert scenario.level in [1, 2, 3]
            assert len(scenario.difficulty_factors) > 0
            assert len(scenario.expected_analysis_points) > 0
    
    def test_ambiguous_request_handling(self):
        """æ›–æ˜§ãªè¦æ±‚ã¸ã®å¯¾å¿œãƒ†ã‚¹ãƒˆ"""
        
        # ãƒ¬ãƒ™ãƒ«3ã§æ›–æ˜§ãªã‚·ãƒŠãƒªã‚ªã‚’ç”Ÿæˆ
        scenario = self.generator.generate_scenario(level=3, category="ambiguous_analysis")
        
        # æ›–æ˜§ãªè¦æ±‚ã«å¯¾ã™ã‚‹é©åˆ‡ãªå›ç­”ä¾‹
        good_ambiguous_response = """
        ã”è¦æ±‚ã«ã¤ã„ã¦ã€ã‚ˆã‚Šå…·ä½“çš„ã«ãŠæ‰‹ä¼ã„ã™ã‚‹ãŸã‚ã«ç¢ºèªã•ã›ã¦ãã ã•ã„ï¼š
        
        ## ç¢ºèªäº‹é …
        1. **æ”¹å–„ã®è¦³ç‚¹**: ã©ã®è¦³ç‚¹ã§ã®æ”¹å–„ã‚’ãŠæ±‚ã‚ã§ã—ã‚‡ã†ã‹ï¼Ÿ
           - æ€§èƒ½ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
           - ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ãƒ»ä¿å®ˆæ€§
           - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
           - ãƒã‚°ä¿®æ­£
        
        2. **å„ªå…ˆåº¦**: æœ€ã‚‚é‡è¦ãªæ”¹å–„ç‚¹ã¯ã©ã¡ã‚‰ã§ã—ã‚‡ã†ã‹ï¼Ÿ
        
        3. **åˆ¶ç´„æ¡ä»¶**: æ”¹å–„æ™‚ã«è€ƒæ…®ã™ã¹ãåˆ¶ç´„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
        
        ## ç¾æ™‚ç‚¹ã§ã®åŒ…æ‹¬çš„åˆ†æ
        ã¨ã‚Šã‚ãˆãšã€ä»¥ä¸‹ã®è¦³ç‚¹ã§åˆ†æã„ãŸã—ã¾ã™ï¼š
        
        **ã‚³ãƒ¼ãƒ‰æ§‹é€ **: [æ§‹é€ åˆ†æçµæœ]
        **æ½œåœ¨çš„å•é¡Œ**: [å•é¡Œç‚¹ã®åˆ—æŒ™]
        **æ”¹å–„å€™è£œ**: [å„ªå…ˆåº¦ä»˜ãã®æ”¹å–„ææ¡ˆ]
        """
        
        evaluation = self.evaluator.evaluate_analysis(scenario, good_ambiguous_response)
        
        # æ›–æ˜§ãªè¦æ±‚ã«å¯¾ã™ã‚‹é©åˆ‡ãªå¯¾å¿œã¯é«˜ãè©•ä¾¡ã•ã‚Œã‚‹ã¹ã
        if scenario.category == "ambiguous_analysis":
            assert evaluation.category_scores.get("ambiguous_analysis", 0) > 0.6
    
    def test_edge_case_files(self):
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
        
        # å„ç¨®ç‰¹æ®Šãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ã‚·ãƒŠãƒªã‚ªç”Ÿæˆãƒ†ã‚¹ãƒˆ
        for _ in range(5):
            scenario = self.generator.generate_scenario()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert Path(scenario.file_path).exists()
            
            # ã‚·ãƒŠãƒªã‚ªãŒé©åˆ‡ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert scenario.user_request is not None
            assert len(scenario.expected_analysis_points) > 0
            assert scenario.level in [1, 2, 3]

class TestFileAnalysisWorkflow:
    """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®ãƒ†ã‚¹ãƒˆ"""
    
    def test_end_to_end_workflow(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        
        # 1. ã‚·ãƒŠãƒªã‚ªç”Ÿæˆ
        generator = FileAnalysisScenarioGenerator()
        batch = generator.create_scenario_batch(batch_size=3)
        
        # 2. æ¨¡æ“¬AIå¿œç­”ç”Ÿæˆ
        mock_responses = [
            # ãƒ¬ãƒ™ãƒ«1ç›¸å½“ã®å›ç­”
            "ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç°¡å˜ãªé–¢æ•°ã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚calculate_areaã¨ã„ã†é–¢æ•°ãŒã‚ã‚Šã¾ã™ã€‚",
            
            # ãƒ¬ãƒ™ãƒ«2ç›¸å½“ã®å›ç­”  
            """
            ã‚³ãƒ¼ãƒ‰åˆ†æï¼š
            - è¤‡æ•°ã®ã‚¯ãƒ©ã‚¹å®šç¾©ã‚ã‚Š
            - ç¶™æ‰¿é–¢ä¿‚ã‚’ä½¿ç”¨
            - å•é¡Œ: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³
            - æ”¹å–„æ¡ˆ: try-catchè¿½åŠ 
            """,
            
            # ãƒ¬ãƒ™ãƒ«3ç›¸å½“ã®å›ç­”
            """
            åŒ…æ‹¬çš„åˆ†æçµæœï¼š
            1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©•ä¾¡: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘è¨­è¨ˆ
            2. å•é¡Œç‰¹å®š: 5ã¤ã®ä¸»è¦å•é¡Œã‚’ç™ºè¦‹
            3. æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—: æ®µéšçš„å®Ÿè£…è¨ˆç”»
            4. è³ªå•: å…·ä½“çš„ãªæ”¹å–„å„ªå…ˆåº¦ã¯ï¼Ÿ
            """
        ]
        
        # 3. è©•ä¾¡å®Ÿè¡Œ
        evaluator = FileAnalysisEvaluator()
        scenarios_and_responses = list(zip(batch["scenarios"], mock_responses))
        evaluation_result = evaluator.batch_evaluate(scenarios_and_responses)
        
        # 4. çµæœæ¤œè¨¼
        assert evaluation_result["statistics"]["total_evaluations"] == 3
        assert "average_score" in evaluation_result["statistics"]
        
        # ã‚¹ã‚³ã‚¢ãŒå¦¥å½“ãªç¯„å›²å†…
        avg_score = evaluation_result["statistics"]["average_score"]
        assert 0 <= avg_score <= 1
        
        # å€‹åˆ¥çµæœã®æ¤œè¨¼
        for result in evaluation_result["individual_results"]:
            assert isinstance(result, AnalysisEvaluationResult)
            assert result.total_score >= 0
            assert len(result.category_scores) > 0
    
    def test_json_serialization(self):
        """JSON ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
        
        # ã‚·ãƒŠãƒªã‚ªç”Ÿæˆã¨è©•ä¾¡
        generator = FileAnalysisScenarioGenerator()
        evaluator = FileAnalysisEvaluator()
        
        scenario = generator.generate_scenario()
        response = "ãƒ†ã‚¹ãƒˆå›ç­”: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯é–¢æ•°ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚"
        evaluation = evaluator.evaluate_analysis(scenario, response)
        
        # JSON ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
        try:
            # asdict ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            from dataclasses import asdict
            evaluation_dict = asdict(evaluation)
            json_str = json.dumps(evaluation_dict, ensure_ascii=False, default=str)
            
            # ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
            parsed = json.loads(json_str)
            
            assert parsed["scenario_id"] == evaluation.scenario_id
            assert parsed["total_score"] == evaluation.total_score
            
        except Exception as e:
            pytest.fail(f"JSON serialization failed: {e}")

if __name__ == "__main__":
    # æ‰‹å‹•ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_instance = TestFileAnalysisIntegration()
    test_instance.setup_method()
    
    print("=== PromptSmith ãƒ•ã‚¡ã‚¤ãƒ«è§£æçµ±åˆãƒ†ã‚¹ãƒˆ ===")
    
    try:
        test_instance.test_scenario_generation()
        print("âœ… ã‚·ãƒŠãƒªã‚ªç”Ÿæˆãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        
        test_instance.test_evaluation_system()  
        print("âœ… è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        
        test_instance.test_batch_evaluation()
        print("âœ… ãƒãƒƒãƒè©•ä¾¡ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        
        workflow_test = TestFileAnalysisWorkflow()
        workflow_test.test_end_to_end_workflow()
        print("âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        
        print("\nğŸ‰ å…¨ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        raise