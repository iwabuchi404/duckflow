#!/usr/bin/env python3
"""
Phase 2: åŸºæœ¬çš„ãªLLMçµ±åˆã®ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import tempfile
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
import sys
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from companion.prompts.base_prompt_generator import BasePromptGenerator
from companion.prompts.main_prompt_generator import MainPromptGenerator
from companion.prompts.context_assembler import ContextAssembler
from companion.prompts.llm_call_manager import LLMCallManager
from companion.prompts.integrated_prompt_system import IntegratedPromptSystem


def test_base_prompt_generator():
    """BasePromptGeneratorã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª BasePromptGeneratorã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # åˆæœŸåŒ–
        generator = BasePromptGenerator()
        print("âœ… BasePromptGeneratoråˆæœŸåŒ–æˆåŠŸ")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®è¨­å®š
        generator.update_session_id("test_session_001")
        
        # ä¼šè©±å±¥æ­´ã®è¿½åŠ 
        generator.add_conversation("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿè£…è¨ˆç”»ã‚’è¦æ±‚")
        generator.add_conversation("ãƒ—ãƒ©ãƒ³ä½œæˆå®Œäº†ã€æ‰¿èªå¾…ã¡")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = generator.generate()
        print(f"ğŸ“Š Base Promptç”Ÿæˆå®Œäº†: {len(prompt)}æ–‡å­—")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã®ç¢ºèª
        if "DuckFlow AI Assistant" in prompt:
            print("âœ… åŸºæœ¬äººæ ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ åŸºæœ¬äººæ ¼ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "å®‰å…¨ç¬¬ä¸€" in prompt:
            print("âœ… å®‰å…¨åŸå‰‡ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ å®‰å…¨åŸå‰‡ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "test_session_001" in prompt:
            print("âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        print("âœ… BasePromptGeneratorãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ BasePromptGeneratorãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_main_prompt_generator():
    """MainPromptGeneratorã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª MainPromptGeneratorã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # åˆæœŸåŒ–
        generator = MainPromptGenerator()
        print("âœ… MainPromptGeneratoråˆæœŸåŒ–æˆåŠŸ")
        
        # å›ºå®š5é …ç›®ã®æ›´æ–°
        generator.update_fixed_five_items(
            goal="å®Ÿè£…è¨ˆç”»ã®ä½œæˆã¨å®Ÿè¡Œ",
            why_now="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå³åº§ã«å®Ÿè£…ã‚’è¦æ±‚",
            constraints=["å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®ã¿", "æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¤‰æ›´ã—ãªã„"],
            plan_brief=["ãƒ—ãƒ©ãƒ³ä½œæˆ", "æ‰¿èªè¦æ±‚", "å®Ÿè¡Œé–‹å§‹"],
            open_questions=["ã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹ã‹"]
        )
        
        # ç¾åœ¨ã®çŠ¶æ³ã®æ›´æ–°
        generator.update_current_situation(
            step="PLANNING",
            status="IN_PROGRESS",
            ongoing_task="å®Ÿè£…è¨ˆç”»ã®ä½œæˆ"
        )
        
        # ä¼šè©±å±¥æ­´ã®è¿½åŠ 
        generator.add_conversation("å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„", "ãƒ—ãƒ©ãƒ³ä½œæˆã‚’é–‹å§‹ã—ã¾ã™")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = generator.generate()
        print(f"ğŸ“Š Main Promptç”Ÿæˆå®Œäº†: {len(prompt)}æ–‡å­—")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã®ç¢ºèª
        if "ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—: PLANNING" in prompt:
            print("âœ… ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "ç›®æ¨™: å®Ÿè£…è¨ˆç”»ã®ä½œæˆã¨å®Ÿè¡Œ" in prompt:
            print("âœ… å›ºå®š5é …ç›®ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ å›ºå®š5é …ç›®ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„" in prompt:
            print("âœ… å‡ºåŠ›æŒ‡ç¤ºãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ å‡ºåŠ›æŒ‡ç¤ºãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        print("âœ… MainPromptGeneratorãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ MainPromptGeneratorãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_context_assembler():
    """ContextAssemblerã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ContextAssemblerã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # åˆæœŸåŒ–
        assembler = ContextAssembler()
        print("âœ… ContextAssembleråˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®AgentState
        test_agent_state = {
            'step': 'PLANNING',
            'status': 'IN_PROGRESS',
            'goal': 'å®Ÿè£…è¨ˆç”»ã®ä½œæˆ',
            'why_now': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚',
            'constraints': ['å®‰å…¨ãªæ“ä½œã®ã¿'],
            'plan_brief': ['ãƒ—ãƒ©ãƒ³ä½œæˆ', 'æ‰¿èªè¦æ±‚'],
            'open_questions': ['å„ªå…ˆé †ä½ã¯ï¼Ÿ'],
            'context_refs': ['file:game_doc.md', 'plan:001'],
            'decision_log': ['ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’æ‰¿èª', 'ãƒ—ãƒ©ãƒ³ä½œæˆã‚’é–‹å§‹'],
            'last_delta': 'ã‚¹ãƒ†ãƒƒãƒ—ã‚’PLANNINGã«è¨­å®š'
        }
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ä¼šè©±å±¥æ­´
        test_conversation_history = [
            {'user': 'å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„', 'assistant': 'ãƒ—ãƒ©ãƒ³ä½œæˆã‚’é–‹å§‹ã—ã¾ã™'},
            {'user': 'æ‰¿èªã—ã¾ã™', 'assistant': 'å®Ÿè£…ã‚’é–‹å§‹ã—ã¾ã™'}
        ]
        
        # æ–‡è„ˆæ§‹ç¯‰
        context = assembler.assemble_context(test_agent_state, test_conversation_history)
        print(f"ğŸ“Š æ–‡è„ˆæ§‹ç¯‰å®Œäº†: {len(context)}æ–‡å­—")
        
        # æ–‡è„ˆå†…å®¹ã®ç¢ºèª
        if "## åŸºæœ¬çŠ¶æ…‹" in context:
            print("âœ… åŸºæœ¬çŠ¶æ…‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ åŸºæœ¬çŠ¶æ…‹ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "## å›ºå®š5é …ç›®" in context:
            print("âœ… å›ºå®š5é …ç›®ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ å›ºå®š5é …ç›®ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "## é–¢é€£å‚ç…§" in context:
            print("âœ… é–¢é€£å‚ç…§ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ é–¢é€£å‚ç…§ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        print("âœ… ContextAssemblerãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ContextAssemblerãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_call_manager():
    """LLMCallManagerã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª LLMCallManagerã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # åˆæœŸåŒ–
        manager = LLMCallManager()
        print("âœ… LLMCallManageråˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        test_prompt = "å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        
        # LLMå‘¼ã³å‡ºã—ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        response = manager.call_llm(test_prompt, expected_format="json")
        print(f"ğŸ“Š LLMå‘¼ã³å‡ºã—å®Œäº†: {response.get('success', False)}")
        
        # å¿œç­”å†…å®¹ã®ç¢ºèª
        if response.get('success'):
            print("âœ… LLMå‘¼ã³å‡ºã—ãŒæˆåŠŸ")
            
            content = response.get('content', '')
            if 'rationale' in content:
                print("âœ… é©åˆ‡ãªå¿œç­”å½¢å¼")
            else:
                print("âŒ ä¸é©åˆ‡ãªå¿œç­”å½¢å¼")
        else:
            print(f"âŒ LLMå‘¼ã³å‡ºã—ãŒå¤±æ•—: {response.get('error')}")
        
        # çµ±è¨ˆæƒ…å ±ã®ç¢ºèª
        stats = manager.get_call_statistics()
        print(f"ğŸ“Š å‘¼ã³å‡ºã—çµ±è¨ˆ: {stats['total_calls']}å›, æˆåŠŸç‡: {stats['success_rate']:.1%}")
        
        print("âœ… LLMCallManagerãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ LLMCallManagerãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_integrated_prompt_system():
    """IntegratedPromptSystemã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª IntegratedPromptSystemã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # åˆæœŸåŒ–
        system = IntegratedPromptSystem()
        print("âœ… IntegratedPromptSystemåˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®AgentState
        test_agent_state = {
            'step': 'PLANNING',
            'status': 'IN_PROGRESS',
            'goal': 'å®Ÿè£…è¨ˆç”»ã®ä½œæˆ',
            'why_now': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚',
            'constraints': ['å®‰å…¨ãªæ“ä½œã®ã¿'],
            'plan_brief': ['ãƒ—ãƒ©ãƒ³ä½œæˆ', 'æ‰¿èªè¦æ±‚'],
            'open_questions': ['å„ªå…ˆé †ä½ã¯ï¼Ÿ']
        }
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
        test_session_data = {
            'session_id': 'test_session_002',
            'total_conversations': 5
        }
        
        # Base + Main ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = system.generate_base_main_prompt(test_agent_state, session_data=test_session_data)
        print(f"ğŸ“Š çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†: {len(prompt)}æ–‡å­—")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã®ç¢ºèª
        if "ã‚ãªãŸã¯DuckFlow AI Assistantã§ã™" in prompt:
            print("âœ… Base PromptãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ Base PromptãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "ç¾åœ¨ã®å¯¾è©±çŠ¶æ³" in prompt:
            print("âœ… Main PromptãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ Main PromptãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        if "=" * 50 in prompt:
            print("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŒºåˆ‡ã‚ŠãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        else:
            print("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŒºåˆ‡ã‚ŠãŒå«ã¾ã‚Œã¦ã„ãªã„")
        
        # LLMå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ
        response = system.call_llm_with_prompt(test_agent_state, session_data=test_session_data)
        print(f"ğŸ“Š LLMå‘¼ã³å‡ºã—å®Œäº†: {response.get('success', False)}")
        
        # çµ±è¨ˆæƒ…å ±ã®ç¢ºèª
        stats = system.get_prompt_statistics()
        print(f"ğŸ“Š ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ±è¨ˆ: Base={stats['base_prompt_length']}æ–‡å­—, Main={stats['main_prompt_length']}æ–‡å­—")
        
        print("âœ… IntegratedPromptSystemãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ IntegratedPromptSystemãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    print("ğŸš€ Phase 2: åŸºæœ¬çš„ãªLLMçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ
    test_results = []
    
    test_results.append(test_base_prompt_generator())
    test_results.append(test_main_prompt_generator())
    test_results.append(test_context_assembler())
    test_results.append(test_llm_call_manager())
    test_results.append(test_integrated_prompt_system())
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    
    # çµæœã‚µãƒãƒªãƒ¼
    success_count = sum(test_results)
    total_count = len(test_results)
    
    print(f"\nğŸ“‹ ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_count} æˆåŠŸ")
    
    if success_count == total_count:
        print("ğŸ¯ Phase 2å®Ÿè£…å®Œäº†ï¼")
        print("\nâœ… å®Ÿè£…å®Œäº†ã—ãŸæ©Ÿèƒ½:")
        print("   - BasePromptGenerator: åŸºæœ¬äººæ ¼ã¨åˆ¶ç´„ã®ç”Ÿæˆ")
        print("   - MainPromptGenerator: å›ºå®š5é …ç›®ã¨ä¼šè©±çŠ¶æ³ã®ç”Ÿæˆ")
        print("   - ContextAssembler: AgentStateã‹ã‚‰ã®æ–‡è„ˆæ§‹ç¯‰")
        print("   - LLMCallManager: åŸºæœ¬çš„ãªLLMå‘¼ã³å‡ºã—")
        print("   - IntegratedPromptSystem: Base + Main ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ±åˆ")
        
        print("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("   - Phase 3: æ©Ÿèƒ½æ‹¡å¼µï¼ˆSpecialized Promptã€ToolRouterï¼‰")
        print("   - å®Ÿéš›ã®LLM APIã¨ã®çµ±åˆ")
        print("   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–")
    else:
        print("âŒ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        print("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
