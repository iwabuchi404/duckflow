"""
Evaluation Node Enhancer - è©•ä¾¡ãƒãƒ¼ãƒ‰å¼·åŒ–

å“è³ªã‚²ãƒ¼ãƒˆ & å¸ä»¤å¡”æ©Ÿèƒ½ã‚’æä¾›
å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³çµæœã‚’è©•ä¾¡ã—ã€Duck Vitals Systemã¨é€£æºã—ã¦æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
"""

from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from enum import Enum

from ..state.agent_state import AgentState
from ..prompts.four_node_context import (
    UnderstandingResult, GatheredInfo, ExecutionResult, EvaluationResult, NextAction
)
from ..services.task_classifier import TaskProfileType
from ..ui.rich_ui import rich_ui
from ..base.llm_client import llm_manager


class EvaluationCriteria(Enum):
    """è©•ä¾¡åŸºæº–"""
    COMPLETENESS = "completeness"        # å®Œå…¨æ€§
    ACCURACY = "accuracy"               # æ­£ç¢ºæ€§
    RELEVANCE = "relevance"             # é–¢é€£æ€§
    SAFETY = "safety"                   # å®‰å…¨æ€§
    EFFICIENCY = "efficiency"           # åŠ¹ç‡æ€§


class QualityGate:
    """å“è³ªã‚²ãƒ¼ãƒˆ - å‡¦ç†å“è³ªã®æœ€çµ‚åˆ¤å®š"""
    
    def __init__(self):
        """å“è³ªã‚²ãƒ¼ãƒˆã‚’åˆæœŸåŒ–"""
        self.minimum_scores = {
            EvaluationCriteria.COMPLETENESS: 0.6,
            EvaluationCriteria.ACCURACY: 0.7,
            EvaluationCriteria.RELEVANCE: 0.5,
            EvaluationCriteria.SAFETY: 0.9,
            EvaluationCriteria.EFFICIENCY: 0.4
        }
        
    def evaluate_quality(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo], 
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> Dict[EvaluationCriteria, float]:
        """ç·åˆå“è³ªè©•ä¾¡
        
        Args:
            understanding_result: ç†è§£çµæœ
            gathered_info: åé›†æƒ…å ±
            execution_result: å®Ÿè¡Œçµæœ
            task_profile_type: TaskProfileåˆ†é¡
            
        Returns:
            è©•ä¾¡åŸºæº–åˆ¥ã‚¹ã‚³ã‚¢è¾æ›¸
        """
        scores = {}
        
        # å®Œå…¨æ€§è©•ä¾¡
        scores[EvaluationCriteria.COMPLETENESS] = self._evaluate_completeness(
            understanding_result, gathered_info, execution_result
        )
        
        # æ­£ç¢ºæ€§è©•ä¾¡
        scores[EvaluationCriteria.ACCURACY] = self._evaluate_accuracy(
            understanding_result, gathered_info, execution_result
        )
        
        # é–¢é€£æ€§è©•ä¾¡
        scores[EvaluationCriteria.RELEVANCE] = self._evaluate_relevance(
            understanding_result, gathered_info, task_profile_type
        )
        
        # å®‰å…¨æ€§è©•ä¾¡
        scores[EvaluationCriteria.SAFETY] = self._evaluate_safety(
            understanding_result, execution_result
        )
        
        # åŠ¹ç‡æ€§è©•ä¾¡
        scores[EvaluationCriteria.EFFICIENCY] = self._evaluate_efficiency(
            understanding_result, gathered_info, execution_result
        )
        
        return scores
    
    def passes_quality_gate(self, quality_scores: Dict[EvaluationCriteria, float]) -> bool:
        """å“è³ªã‚²ãƒ¼ãƒˆé€šéåˆ¤å®š
        
        Args:
            quality_scores: è©•ä¾¡åŸºæº–åˆ¥ã‚¹ã‚³ã‚¢
            
        Returns:
            å“è³ªã‚²ãƒ¼ãƒˆé€šéå¯å¦
        """
        for criteria, score in quality_scores.items():
            minimum_required = self.minimum_scores[criteria]
            if score < minimum_required:
                rich_ui.print_warning(f"å“è³ªã‚²ãƒ¼ãƒˆæœªé€šé: {criteria.value} = {score:.2f} < {minimum_required}")
                return False
        
        rich_ui.print_success("ğŸ‰ å“è³ªã‚²ãƒ¼ãƒˆé€šé")
        return True
    
    def _evaluate_completeness(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo], 
        execution_result: Optional[ExecutionResult]
    ) -> float:
        """å®Œå…¨æ€§è©•ä¾¡"""
        completeness_score = 0.0
        total_components = 3
        
        # ç†è§£çµæœã®å®Œå…¨æ€§
        if understanding_result and understanding_result.execution_plan:
            if understanding_result.execution_plan.steps and understanding_result.execution_plan.required_tools:
                completeness_score += 0.4
        
        # æƒ…å ±åé›†ã®å®Œå…¨æ€§
        if gathered_info:
            if gathered_info.collected_files:
                completeness_score += 0.3
            if gathered_info.project_context:
                completeness_score += 0.1
        
        # å®Ÿè¡Œçµæœã®å®Œå…¨æ€§
        if execution_result and execution_result.success:
            completeness_score += 0.2
        
        return min(completeness_score, 1.0)
    
    def _evaluate_accuracy(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo], 
        execution_result: Optional[ExecutionResult]
    ) -> float:
        """æ­£ç¢ºæ€§è©•ä¾¡"""
        accuracy_indicators = []
        
        # ç†è§£çµæœã®æ­£ç¢ºæ€§
        if understanding_result:
            accuracy_indicators.append(understanding_result.confidence)
        
        # åé›†ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§
        if gathered_info and gathered_info.collected_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚ŠæˆåŠŸç‡
            successful_reads = sum(
                1 for file_content in gathered_info.collected_files.values()
                if hasattr(file_content, 'content') and 
                not file_content.content.startswith('[èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼')
            )
            total_files = len(gathered_info.collected_files)
            if total_files > 0:
                read_success_rate = successful_reads / total_files
                accuracy_indicators.append(read_success_rate)
        
        # å®Ÿè¡Œçµæœã®æ­£ç¢ºæ€§
        if execution_result:
            execution_accuracy = 1.0 if execution_result.success else 0.3
            accuracy_indicators.append(execution_accuracy)
        
        return sum(accuracy_indicators) / len(accuracy_indicators) if accuracy_indicators else 0.5
    
    def _evaluate_relevance(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo], 
        task_profile_type: Optional[TaskProfileType]
    ) -> float:
        """é–¢é€£æ€§è©•ä¾¡"""
        if not task_profile_type:
            return 0.5
        
        relevance_score = 0.0
        
        # TaskProfileã¨ç†è§£çµæœã®é–¢é€£æ€§
        if understanding_result:
            relevance_score += 0.4
        
        # TaskProfileã¨åé›†ãƒ‡ãƒ¼ã‚¿ã®é–¢é€£æ€§
        if gathered_info and gathered_info.collected_files:
            # TaskProfileã«å¿œã˜ãŸé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if task_profile_type in [TaskProfileType.FILE_ANALYSIS, TaskProfileType.CODE_EXPLANATION]:
                if any('.py' in str(path) or 'test' in str(path) 
                      for path in gathered_info.collected_files.keys()):
                    relevance_score += 0.6
            else:
                relevance_score += 0.4
        
        return min(relevance_score, 1.0)
    
    def _evaluate_safety(
        self, 
        understanding_result: Optional[UnderstandingResult],
        execution_result: Optional[ExecutionResult]
    ) -> float:
        """å®‰å…¨æ€§è©•ä¾¡"""
        safety_score = 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é«˜å®‰å…¨æ€§
        
        # å®Ÿè¡Œãƒªã‚¹ã‚¯ã«ã‚ˆã‚‹å®‰å…¨æ€§æ¸›ç‚¹
        if execution_result and execution_result.risk_assessment:
            if execution_result.risk_assessment.overall_risk.value == "high":
                safety_score -= 0.3
            elif execution_result.risk_assessment.overall_risk.value == "medium":
                safety_score -= 0.1
        
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã«ã‚ˆã‚‹å®‰å…¨æ€§æ¸›ç‚¹
        if execution_result and execution_result.errors:
            error_penalty = min(len(execution_result.errors) * 0.1, 0.3)
            safety_score -= error_penalty
        
        return max(safety_score, 0.0)
    
    def _evaluate_efficiency(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo], 
        execution_result: Optional[ExecutionResult]
    ) -> float:
        """åŠ¹ç‡æ€§è©•ä¾¡"""
        efficiency_indicators = []
        
        # ç†è§£æ®µéšã®åŠ¹ç‡æ€§
        if understanding_result:
            # è¤‡é›‘åº¦äºˆæ¸¬ã¨å®Ÿéš›ã®ä¸€è‡´åº¦
            predicted_complexity = understanding_result.execution_plan.estimated_complexity
            complexity_score = {"low": 0.9, "medium": 0.7, "high": 0.5}.get(predicted_complexity, 0.6)
            efficiency_indicators.append(complexity_score)
        
        # æƒ…å ±åé›†ã®åŠ¹ç‡æ€§
        if gathered_info and gathered_info.collected_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ã®ãƒãƒ©ãƒ³ã‚¹
            file_count = len(gathered_info.collected_files)
            if 1 <= file_count <= 10:  # é©åˆ‡ãªç¯„å›²
                efficiency_indicators.append(0.8)
            else:
                efficiency_indicators.append(0.5)
        
        # å®Ÿè¡Œã®åŠ¹ç‡æ€§
        if execution_result:
            execution_time = execution_result.execution_time
            if execution_time < 1.0:  # 1ç§’ä»¥ä¸‹
                efficiency_indicators.append(0.9)
            elif execution_time < 5.0:  # 5ç§’ä»¥ä¸‹
                efficiency_indicators.append(0.7)
            else:
                efficiency_indicators.append(0.4)
        
        return sum(efficiency_indicators) / len(efficiency_indicators) if efficiency_indicators else 0.5


class EvaluationNodeEnhancer:
    """è©•ä¾¡ãƒãƒ¼ãƒ‰å¼·åŒ–æ©Ÿèƒ½
    
    å“è³ªã‚²ãƒ¼ãƒˆ & å¸ä»¤å¡”ã¨ã—ã¦ã€å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³çµæœã‚’è©•ä¾¡ã—ã€
    Duck Vitals Systemã¨é€£æºã—ã¦æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
    """
    
    def __init__(self):
        """è©•ä¾¡ãƒãƒ¼ãƒ‰å¼·åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ–"""
        self.quality_gate = QualityGate()
    
    async def enhance_evaluation(
        self, 
        state_obj: AgentState,
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> EvaluationResult:
        """å¼·åŒ–ã•ã‚ŒãŸè©•ä¾¡å®Ÿè¡Œ
        
        Args:
            state_obj: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçŠ¶æ…‹
            understanding_result: ç†è§£çµæœ
            gathered_info: åé›†æƒ…å ±
            execution_result: å®Ÿè¡Œçµæœ
            task_profile_type: TaskProfileåˆ†é¡
            
        Returns:
            å¼·åŒ–ã•ã‚ŒãŸè©•ä¾¡çµæœ
        """
        try:
            rich_ui.print_step("ğŸ¯ [å“è³ªã‚²ãƒ¼ãƒˆ & å¸ä»¤å¡”] å¼·åŒ–è©•ä¾¡å®Ÿè¡Œ")
            
            # 1. å“è³ªè©•ä¾¡å®Ÿè¡Œ
            quality_scores = self.quality_gate.evaluate_quality(
                understanding_result, gathered_info, execution_result, task_profile_type
            )
            
            # 2. å“è³ªã‚²ãƒ¼ãƒˆåˆ¤å®š
            quality_gate_passed = self.quality_gate.passes_quality_gate(quality_scores)
            overall_quality_score = sum(quality_scores.values()) / len(quality_scores)
            
            # 3. Duck Vitals System çµ±åˆè©•ä¾¡
            vitals_assessment = self._assess_duck_vitals(state_obj, quality_scores)
            
            # 4. LLMæ¨ç†ã«ã‚ˆã‚‹è©³ç´°è©•ä¾¡
            llm_evaluation = await self._perform_llm_reasoning(
                state_obj, understanding_result, gathered_info, execution_result, 
                task_profile_type, quality_scores
            )
            
            # 5. æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š (å¸ä»¤å¡”æ©Ÿèƒ½)
            next_action = self._determine_next_action(
                state_obj, quality_gate_passed, quality_scores, 
                understanding_result, gathered_info, execution_result, vitals_assessment
            )
            
            # 6. å¿œç­”ç”Ÿæˆæº–å‚™åº¦ãƒã‚§ãƒƒã‚¯
            response_readiness, template_completeness = self._assess_response_generation_readiness(
                understanding_result, gathered_info, execution_result, task_profile_type
            )
            
            # 7. çµ±åˆè©•ä¾¡çµæœä½œæˆ
            evaluation_result = EvaluationResult(
                overall_quality_score=overall_quality_score,
                task_completion_status=self._determine_completion_status(
                    understanding_result, gathered_info, execution_result
                ),
                identified_issues=self._identify_issues(quality_scores, state_obj),
                recommended_next_action=next_action,
                confidence_in_recommendation=self._calculate_recommendation_confidence(quality_scores),
                reasoning=llm_evaluation.get("reasoning", "å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡ã«åŸºã¥ãåˆ¤å®š"),
                duck_vitals_assessment=vitals_assessment,
                response_generation_readiness=response_readiness,
                template_data_completeness=template_completeness,
                quality_gate_passed=quality_gate_passed
            )
            
            rich_ui.print_success(f"ğŸ¯ è©•ä¾¡å®Œäº†: æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ = {next_action.value}")
            
            return evaluation_result
            
        except Exception as e:
            rich_ui.print_error(f"å¼·åŒ–è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡çµæœ
            return EvaluationResult(
                overall_quality_score=0.3,
                task_completion_status="error",
                identified_issues=[f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}"],
                recommended_next_action=NextAction.RESPONSE_GENERATION,
                confidence_in_recommendation=0.5,
                reasoning=f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {str(e)}",
                duck_vitals_assessment={"mood": 0.5, "focus": 0.5, "stamina": 0.5}
            )
    
    def _assess_duck_vitals(self, state_obj: AgentState, quality_scores: Dict[EvaluationCriteria, float]) -> Dict[str, float]:
        """Duck Vitals System è©•ä¾¡"""
        return {
            "mood": state_obj.vitals.mood,
            "focus": state_obj.vitals.focus,
            "stamina": state_obj.vitals.stamina,
            "health_status": state_obj.vitals.get_health_status(),
            "quality_alignment": sum(quality_scores.values()) / len(quality_scores)
        }
    
    async def _perform_llm_reasoning(
        self, 
        state_obj: AgentState,
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType],
        quality_scores: Dict[EvaluationCriteria, float]
    ) -> Dict[str, Any]:
        """LLMæ¨ç†ã«ã‚ˆã‚‹è©³ç´°è©•ä¾¡"""
        try:
            # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self._build_llm_evaluation_prompt(
                understanding_result, gathered_info, execution_result, 
                task_profile_type, quality_scores
            )
            
            # LLMæ¨ç†å®Ÿè¡Œ
            response = llm_manager.chat(prompt)
            
            return {
                "reasoning": response[:300],  # æœ€åˆã®300æ–‡å­—
                "full_response": response
            }
            
        except Exception as e:
            rich_ui.print_warning(f"LLMæ¨ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "reasoning": "LLMæ¨ç†ã‚¨ãƒ©ãƒ¼ã®ãŸã‚å“è³ªã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹è©•ä¾¡",
                "full_response": ""
            }
    
    def _build_llm_evaluation_prompt(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType],
        quality_scores: Dict[EvaluationCriteria, float]
    ) -> str:
        """LLMè©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        prompt_parts = [
            "ä»¥ä¸‹ã®å‡¦ç†çµæœã‚’è©•ä¾¡ã—ã€å“è³ªã‚²ãƒ¼ãƒˆé€šéå¯å¦ã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚",
            "",
            f"TaskProfile: {task_profile_type.value if task_profile_type else 'Unknown'}",
            "",
            "å“è³ªã‚¹ã‚³ã‚¢:"
        ]
        
        for criteria, score in quality_scores.items():
            prompt_parts.append(f"â€¢ {criteria.value}: {score:.2f}")
        
        prompt_parts.extend([
            "",
            "å‡¦ç†çŠ¶æ³:",
            f"â€¢ ç†è§£ãƒ»è¨ˆç”»: {'å®Œäº†' if understanding_result else 'æœªå®Œäº†'}",
            f"â€¢ æƒ…å ±åé›†: {'å®Œäº†' if gathered_info else 'æœªå®Œäº†'}",
            f"â€¢ å®Ÿè¡Œ: {'å®Œäº†' if execution_result else 'æœªå®Œäº†'}",
            "",
            "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å€™è£œ:",
            "1. RESPONSE_GENERATION - å¿œç­”ç”Ÿæˆã¸é€²ã‚€",
            "2. REPLAN - å†è¨ˆç”»ãŒå¿…è¦", 
            "3. COLLECT_MORE_INFO - è¿½åŠ æƒ…å ±åé›†",
            "4. EXECUTE_ADDITIONAL - è¿½åŠ å®Ÿè¡Œ",
            "5. END - å‡¦ç†å®Œäº†",
            "",
            "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ç†ç”±ã‚’ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        ])
        
        return "\n".join(prompt_parts)
    
    def _determine_next_action(
        self, 
        state_obj: AgentState,
        quality_gate_passed: bool,
        quality_scores: Dict[EvaluationCriteria, float],
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        vitals_assessment: Dict[str, float]
    ) -> NextAction:
        """æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š (å¸ä»¤å¡”æ©Ÿèƒ½)"""
        
        # Duck Pacemaker ä»‹å…¥ãƒã‚§ãƒƒã‚¯ (æœ€å„ªå…ˆ)
        intervention = state_obj.needs_duck_intervention()
        if intervention["required"]:
            if intervention["priority"] == "CRITICAL":
                return NextAction.DUCK_CALL
            elif intervention["priority"] == "HIGH" and intervention["action"] == "REPLAN":
                return NextAction.REPLAN
        
        # å“è³ªã‚²ãƒ¼ãƒˆæœªé€šéã®å ´åˆ
        if not quality_gate_passed:
            # å®Œå…¨æ€§ãŒä½ã„å ´åˆ
            if quality_scores.get(EvaluationCriteria.COMPLETENESS, 0) < 0.5:
                if not gathered_info or not gathered_info.collected_files:
                    return NextAction.COLLECT_MORE_INFO
                else:
                    return NextAction.REPLAN
            
            # æ­£ç¢ºæ€§ãŒä½ã„å ´åˆ
            if quality_scores.get(EvaluationCriteria.ACCURACY, 0) < 0.6:
                return NextAction.REPLAN
        
        # æ®µéšçš„å®Œäº†ãƒã‚§ãƒƒã‚¯
        if not understanding_result:
            return NextAction.REPLAN
        elif not gathered_info:
            return NextAction.COLLECT_MORE_INFO
        elif not execution_result:
            return NextAction.EXECUTE_ADDITIONAL
        else:
            # å…¨ã¦å®Œäº†ã—ã¦ã„ã‚‹å ´åˆã¯å¿œç­”ç”Ÿæˆã¸
            return NextAction.RESPONSE_GENERATION
    
    def _assess_response_generation_readiness(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> Tuple[bool, float]:
        """å¿œç­”ç”Ÿæˆæº–å‚™åº¦è©•ä¾¡"""
        readiness_indicators = []
        
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡
        if understanding_result:
            readiness_indicators.append(0.3)
        if gathered_info and gathered_info.collected_files:
            readiness_indicators.append(0.4)
        if execution_result and execution_result.success:
            readiness_indicators.append(0.2)
        if task_profile_type:
            readiness_indicators.append(0.1)
        
        template_completeness = sum(readiness_indicators)
        response_readiness = template_completeness >= 0.6
        
        return response_readiness, template_completeness
    
    def _determine_completion_status(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult]
    ) -> str:
        """å®Œäº†çŠ¶æ³åˆ¤å®š"""
        completed_phases = 0
        total_phases = 3
        
        if understanding_result:
            completed_phases += 1
        if gathered_info:
            completed_phases += 1
        if execution_result and execution_result.success:
            completed_phases += 1
        
        completion_percentage = completed_phases / total_phases
        
        if completion_percentage >= 1.0:
            return "completed"
        elif completion_percentage >= 0.6:
            return "mostly_completed"
        elif completion_percentage >= 0.3:
            return "in_progress"
        else:
            return "just_started"
    
    def _identify_issues(self, quality_scores: Dict[EvaluationCriteria, float], state_obj: AgentState) -> List[str]:
        """å•é¡Œç‰¹å®š"""
        issues = []
        
        # å“è³ªã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹å•é¡Œç‰¹å®š
        for criteria, score in quality_scores.items():
            if score < 0.5:
                issues.append(f"{criteria.value}ã‚¹ã‚³ã‚¢ä½ä¸‹ ({score:.2f})")
        
        # ãƒã‚¤ã‚¿ãƒ«ãƒ™ãƒ¼ã‚¹å•é¡Œç‰¹å®š
        if state_obj.vitals.stamina < 0.2:
            issues.append("ä½“åŠ›ä¸è¶³ã«ã‚ˆã‚‹å‡¦ç†å“è³ªä½ä¸‹")
        if state_obj.vitals.focus < 0.4:
            issues.append("é›†ä¸­åŠ›ä½ä¸‹ã«ã‚ˆã‚‹æ€è€ƒåœæ»")
        if state_obj.vitals.mood < 0.6:
            issues.append("è‡ªä¿¡ä¸è¶³ã«ã‚ˆã‚‹åˆ¤æ–­å›°é›£")
        
        return issues
    
    def _calculate_recommendation_confidence(self, quality_scores: Dict[EvaluationCriteria, float]) -> float:
        """æ¨å¥¨ä¿¡é ¼åº¦è¨ˆç®—"""
        average_quality = sum(quality_scores.values()) / len(quality_scores)
        
        # å“è³ªã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©æ¨å¥¨ã®ä¿¡é ¼åº¦ã‚‚é«˜ã„
        base_confidence = average_quality
        
        # å“è³ªã‚¹ã‚³ã‚¢ã®åˆ†æ•£ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼‰
        quality_values = list(quality_scores.values())
        quality_variance = sum((score - average_quality) ** 2 for score in quality_values) / len(quality_values)
        
        # åˆ†æ•£ãŒä½ã„ï¼ˆä¸€è²«ã—ã¦ã„ã‚‹ï¼‰ã»ã©ä¿¡é ¼åº¦å‘ä¸Š
        consistency_bonus = max(0, 0.2 - quality_variance)
        
        final_confidence = min(base_confidence + consistency_bonus, 1.0)
        return final_confidence


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
evaluation_node_enhancer = EvaluationNodeEnhancer()