"""
Five Node Orchestrator - 5ãƒãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (LangGraphãƒ™ãƒ¼ã‚¹)

è©•ä¾¡ãƒãƒ¼ãƒ‰ä¸­å¿ƒã®å“è³ªä¿è¨¼ãƒ«ãƒ¼ãƒ—ã¨æ±ºå®šè«–çš„å¿œç­”ç”Ÿæˆã‚’å®Ÿç¾
å…¨ã¦ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³çµæœã¯è©•ä¾¡ãƒãƒ¼ãƒ‰ã«é›†ç´„ã•ã‚Œã€ãã“ã§æ¬¡ã®è¡Œå…ˆãŒæ±ºå®šã•ã‚Œã‚‹
LangGraphã«ã‚ˆã‚‹å …ç‰¢ãªã‚¹ãƒ†ãƒ¼ãƒˆãƒã‚·ãƒ³å®Ÿè£…
"""

import asyncio
from typing import Dict, Any, Optional, List, Tuple, Union
from datetime import datetime
from enum import Enum

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph

from ..state.agent_state import AgentState
from ..services.task_classifier import TaskProfileType, task_classifier
from ..prompts.four_node_context import (
    UnderstandingResult, GatheredInfo, ExecutionResult, EvaluationResult, NextAction
)
from ..orchestration.four_node_helpers import FourNodeHelpers
from ..orchestration.response_generation_node import response_generation_node
from ..services.llm_service import llm_service
from ..ui.rich_ui import rich_ui
from ..base.llm_client import llm_manager
from ..tools.duck_scan import duck_scan
from ..keeper import duck_fs

# ã‚·ãƒ³ãƒ—ãƒ«å‹•çš„Duck Pacemaker
from ..pacemaker import SimpleDynamicPacemaker, UserConsultation


class NodeType(Enum):
    """5ãƒãƒ¼ãƒ‰ã®ç¨®é¡"""
    PLANNING = "planning"
    INFORMATION_COLLECTION = "information_collection" 
    SAFE_EXECUTION = "safe_execution"
    EVALUATION_CONTINUATION = "evaluation_continuation"
    RESPONSE_GENERATION = "response_generation"


class FiveNodeOrchestrator:
    """5ï¸âƒ£ãƒãƒ¼ãƒ‰ãƒ»ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ (LangGraphãƒ™ãƒ¼ã‚¹)
    
    è©•ä¾¡ãƒãƒ¼ãƒ‰ã‚’ä¸­å¿ƒã¨ã—ãŸå“è³ªä¿è¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿç¾
    å…¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³çµæœã¯è©•ä¾¡ãƒãƒ¼ãƒ‰ã«é›†ç´„ã•ã‚Œã€æ¬¡ã®è¡Œå‹•ãŒæ±ºå®šã•ã‚Œã‚‹
    LangGraphã«ã‚ˆã‚‹å …ç‰¢ãªã‚¹ãƒ†ãƒ¼ãƒˆãƒã‚·ãƒ³å®Ÿè£…
    """
    
    def __init__(self, state: AgentState):
        """5ãƒãƒ¼ãƒ‰ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ– (4ãƒãƒ¼ãƒ‰äº’æ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹)
        
        Args:
            state: AgentState ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.state = state
        
        # ä¾å­˜ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        from ..orchestration.routing_engine import RoutingEngine
        from ..prompts.four_node_compiler import FourNodePromptCompiler
        
        self.routing_engine = RoutingEngine()
        self.prompt_compiler = FourNodePromptCompiler()
        self.helpers = FourNodeHelpers(self.prompt_compiler, self.routing_engine)
        
        # ã‚·ãƒ³ãƒ—ãƒ«å‹•çš„Duck Pacemakerã®åˆæœŸåŒ–
        self.dynamic_pacemaker = SimpleDynamicPacemaker()
        self.user_consultation = UserConsultation()
        
        # LangGraphã®æ§‹ç¯‰
        self.graph = self._build_langgraph()
        
        # å®Ÿè¡Œçµ±è¨ˆ
        self.execution_stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'average_execution_time': 0.0
        }
        
        # å‹•çš„åˆ¶å¾¡ç”¨ã®çŠ¶æ…‹
        self.current_task_profile: Optional[TaskProfileType] = None
        self.session_start_time: Optional[datetime] = None
        
    def run_conversation(self, user_message: str) -> None:
        """ãƒ¡ã‚¤ãƒ³å¯¾è©±å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ (å‹•çš„Duck Pacemakerçµ±åˆç‰ˆ)
        
        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        try:
            start_time = datetime.now()
            self.session_start_time = start_time
            
            rich_ui.print_header("ğŸ¦† 5-Node Dynamic Pacemaker Orchestration é–‹å§‹")
            
            # ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡
            classification_result = task_classifier.classify(user_message)
            self.current_task_profile = classification_result.profile_type
            
            # å‹•çš„åˆ¶é™è¨­å®š
            pacemaker_result = self.dynamic_pacemaker.start_session(
                state=self.state,
                task_profile=self.current_task_profile
            )
            
            # åˆ¶é™è¨­å®šã®è¡¨ç¤ºï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
            rich_ui.print_message(f"ğŸ¦† {self.current_task_profile.value}ã‚’å®Ÿè¡Œä¸­ã§ã™...", "info")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŠ¶æ…‹ã«è¿½åŠ 
            self.state.add_message("user", user_message)
            
            # LangGraphå®Ÿè¡Œ
            final_state = self.graph.invoke({
                "agent_state": self.state,
                "user_message": user_message,
                "current_node": "planning",
                "loop_count": 0,
                "execution_results": {},
                "task_profile": self.current_task_profile,
                "pacemaker_result": pacemaker_result,
                "final_response": None
            })
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†
            execution_time = (datetime.now() - start_time).total_seconds()
            success = "error" not in final_state
            
            if hasattr(self, 'dynamic_pacemaker') and self.current_task_profile:
                self.dynamic_pacemaker.end_session(
                    state=self.state,
                    success=success,
                    loops_used=self.state.graph_state.loop_count
                )
            
            # å®Ÿè¡Œçµ±è¨ˆæ›´æ–°
            self._update_execution_stats(success, execution_time)
            
            # æœ€çµ‚çŠ¶æ…‹ã‚’åæ˜ 
            if "agent_state" in final_state:
                self.state = final_state["agent_state"]
            
            rich_ui.print_success(f"ğŸ‰ å‹•çš„5ãƒãƒ¼ãƒ‰ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº† ({execution_time:.2f}ç§’)")
            
        except Exception as e:
            rich_ui.print_error(f"å‹•çš„5ãƒãƒ¼ãƒ‰ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†
            if hasattr(self, 'dynamic_pacemaker') and self.current_task_profile:
                self.dynamic_pacemaker.end_session(
                    state=self.state,
                    success=False
                )
            
            self._update_execution_stats(False, 0.0)
            
            # ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’è¿½åŠ 
            error_response = self._generate_error_response(str(e))
            self.state.add_message("assistant", error_response)
    
    async def orchestrate(self, state_obj: AgentState, user_message: str) -> str:
        """5ãƒãƒ¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        Args:
            state_obj: AgentState ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            
        Returns:
            æœ€çµ‚çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘å¿œç­”
        """
        try:
            rich_ui.print_header("ğŸ¦† 5-Node Orchestration é–‹å§‹")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
            self._initialize_session(state_obj, user_message)
            
            # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—: è©•ä¾¡ãƒãƒ¼ãƒ‰ä¸­å¿ƒã®å“è³ªä¿è¨¼ãƒ•ãƒ­ãƒ¼
            current_node = NodeType.PLANNING
            understanding_result = None
            gathered_info = None
            execution_result = None
            task_profile_type = None
            
            while self.loop_count < self.max_loops:
                self.loop_count += 1
                rich_ui.print_message(f"[ãƒ«ãƒ¼ãƒ— {self.loop_count}] ç¾åœ¨ã®ãƒãƒ¼ãƒ‰: {current_node.value}", "info")
                
                # D.U.C.K. Vitals æ›´æ–°
                self._update_duck_vitals(state_obj, current_node)
                
                # ãƒãƒ¼ãƒ‰å®Ÿè¡Œ
                if current_node == NodeType.PLANNING:
                    understanding_result, task_profile_type = await self._execute_planning_node(
                        state_obj, user_message
                    )
                    next_node = NodeType.INFORMATION_COLLECTION
                    
                elif current_node == NodeType.INFORMATION_COLLECTION:
                    gathered_info = await self._execute_information_collection_node(
                        state_obj, understanding_result
                    )
                    next_node = NodeType.SAFE_EXECUTION
                    
                elif current_node == NodeType.SAFE_EXECUTION:
                    execution_result = await self._execute_safe_execution_node(
                        state_obj, understanding_result, gathered_info
                    )
                    next_node = NodeType.EVALUATION_CONTINUATION
                    
                elif current_node == NodeType.EVALUATION_CONTINUATION:
                    # ğŸ¯ ä¸­å¤®åˆ¶å¾¡: å…¨çµæœã‚’è©•ä¾¡ã—ã€æ¬¡ã®è¡Œå…ˆã‚’æ±ºå®š
                    evaluation_result, next_node = await self._execute_evaluation_node(
                        state_obj, understanding_result, gathered_info, execution_result, task_profile_type
                    )
                    
                    # Duck Pacemaker ã«ã‚ˆã‚‹å¼·åˆ¶ä»‹å…¥ãƒã‚§ãƒƒã‚¯
                    intervention = state_obj.needs_duck_intervention()
                    if intervention["required"]:
                        rich_ui.print_warning(f"ğŸ¦† Duck Pacemaker ä»‹å…¥: {intervention['reason']}")
                        if intervention["action"] == "HALT_AND_CONSULT":
                            # å¼·åˆ¶çš„ã«äººé–“ç›¸è«‡ãƒ¢ãƒ¼ãƒ‰ã¸
                            return self._generate_consultation_response(state_obj, intervention)
                        elif intervention["action"] == "REPLAN":
                            # å¼·åˆ¶çš„ã«å†è¨ˆç”»ã¸
                            next_node = NodeType.PLANNING
                            rich_ui.print_message("ğŸ¦† å†è¨ˆç”»ã‚’å¼·åˆ¶å®Ÿè¡Œ", "warning")
                    
                elif current_node == NodeType.RESPONSE_GENERATION:
                    # The Scribe: æ±ºå®šè«–çš„å¿œç­”ç”Ÿæˆ
                    final_response = await self._execute_response_generation_node(
                        state_obj, gathered_info, execution_result, task_profile_type
                    )
                    
                    # ç”Ÿæˆå“è³ªã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ˆè©•ä¾¡ãƒãƒ¼ãƒ‰ã«æˆ»ã‚‹ï¼‰
                    next_node = NodeType.EVALUATION_CONTINUATION
                    state_obj.collected_context["final_response"] = final_response
                
                # ãƒ«ãƒ¼ãƒ—çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if next_node == "END":
                    rich_ui.print_success("ğŸ‰ ã‚¿ã‚¹ã‚¯å®Œäº†")
                    break
                elif next_node == "DUCK_CALL":
                    return self._generate_duck_call_response(state_obj)
                
                # æ¬¡ã®ãƒãƒ¼ãƒ‰ã¸ç§»è¡Œ
                self.current_node = current_node
                current_node = next_node
                
                # ãƒãƒ¼ãƒ‰å®Ÿè¡Œå±¥æ­´ã‚’è¨˜éŒ²
                self._record_node_execution(current_node, {
                    "understanding_result": understanding_result is not None,
                    "gathered_info": gathered_info is not None,
                    "execution_result": execution_result is not None,
                    "loop_count": self.loop_count
                })
            
            # æœ€å¤§ãƒ«ãƒ¼ãƒ—åˆ°é”æ™‚ã®å‡¦ç†
            if self.loop_count >= self.max_loops:
                rich_ui.print_error("âš ï¸ æœ€å¤§ãƒ«ãƒ¼ãƒ—å›æ•°ã«åˆ°é”")
                return self._generate_timeout_response(state_obj)
            
            # æœ€çµ‚å¿œç­”ã®å–å¾—
            if "final_response" in state_obj.collected_context:
                return state_obj.collected_context["final_response"]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç·Šæ€¥å¿œç­”ç”Ÿæˆ
                return await self._execute_response_generation_node(
                    state_obj, gathered_info, execution_result, task_profile_type or TaskProfileType.GENERAL_CHAT
                )
                
        except Exception as e:
            rich_ui.print_error(f"5ãƒãƒ¼ãƒ‰ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_error_response(str(e))
    
    def _build_langgraph(self) -> CompiledStateGraph:
        """LangGraphãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒ†ãƒ¼ãƒˆãƒã‚·ãƒ³ã‚’æ§‹ç¯‰
        
        Returns:
            CompiledStateGraph: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã®LangGraph
        """
        # ã‚¹ãƒ†ãƒ¼ãƒˆã‚¹ã‚­ãƒ¼ãƒå®šç¾© (TypedDictã‚’ä½¿ç”¨)
        from typing import TypedDict
        
        class FiveNodeState(TypedDict):
            agent_state: AgentState
            user_message: str
            current_node: str
            loop_count: int
            execution_results: dict
            final_response: Optional[str]
        
        workflow = StateGraph(FiveNodeState)
        
        # ãƒãƒ¼ãƒ‰å®šç¾©
        workflow.add_node("planning", self._planning_node)
        workflow.add_node("information_collection", self._information_collection_node)
        workflow.add_node("safe_execution", self._safe_execution_node)
        workflow.add_node("evaluation_continuation", self._evaluation_continuation_node)
        workflow.add_node("response_generation", self._response_generation_node)
        
        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
        workflow.set_entry_point("planning")
        
        # ã‚¨ãƒƒã‚¸å®šç¾©ï¼ˆæ¡ä»¶åˆ†å²ï¼‰
        workflow.add_conditional_edges(
            "planning",
            self._after_planning,
            {
                "information_collection": "information_collection",
                "safe_execution": "safe_execution",
                "response_generation": "response_generation",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "information_collection", 
            self._after_information_collection,
            {
                "safe_execution": "safe_execution",
                "evaluation_continuation": "evaluation_continuation",
                "planning": "planning",
                "end": END
            }
        )
        
        workflow.add_edge("safe_execution", "evaluation_continuation")
        
        workflow.add_conditional_edges(
            "evaluation_continuation",
            self._after_evaluation_continuation,
            {
                "planning": "planning",
                "information_collection": "information_collection", 
                "safe_execution": "safe_execution",
                "response_generation": "response_generation",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "response_generation",
            self._after_response_generation,
            {
                "evaluation_continuation": "evaluation_continuation",
                "end": END
            }
        )
        
        return workflow.compile()
    
    # === LangGraphãƒãƒ¼ãƒ‰å®Ÿè£… ===
    
    def _planning_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """1ï¸âƒ£ ç†è§£ãƒ»è¨ˆç”»ãƒãƒ¼ãƒ‰ (The Architect) - LangGraphç‰ˆ"""
        try:
            rich_ui.print_step("[The Architect] ç†è§£ãƒ»è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º")
            
            agent_state = state["agent_state"]
            user_message = state["user_message"]
            
            # è»½é‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
            self.helpers.prepare_lightweight_context(agent_state)
            
            # æ„å›³åˆ†æ (RoutingEngine)
            routing_decision = self.helpers.analyze_user_intent(agent_state)
            
            # TaskProfileåˆ†é¡
            classification_result = task_classifier.classify(user_message)
            task_profile_type = classification_result.profile_type
            
            rich_ui.print_message(f"TaskProfile: {task_profile_type.value} (ä¿¡é ¼åº¦: {classification_result.confidence:.2f})", "info")
            
            # å†è©¦è¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®š
            is_retry = self.helpers.is_retry_context(agent_state)
            
            # Four Node Context ä½œæˆ
            four_node_context = self._create_four_node_context(agent_state, routing_decision, task_profile_type)
            
            # ç†è§£ãƒ»è¨ˆç”»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ
            understanding_result = self.helpers.execute_understanding_prompt(
                agent_state, four_node_context, routing_decision, is_retry
            )
            
            # ã€ä¿®æ­£ã€‘The Pecking Order ã®æ§‹ç¯‰ãƒ»æ›´æ–° (åŒæœŸç‰ˆ)
            try:
                # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
                import asyncio
                loop = None
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # æ—¢ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå‹•ä½œä¸­ã®å ´åˆã¯ã€ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã›ãšã«è­¦å‘Šã®ã¿
                        rich_ui.print_warning("The Pecking Order: ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å‹•ä½œä¸­ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    else:
                        loop.run_until_complete(self._build_or_update_pecking_order(
                            agent_state, understanding_result, is_retry, task_profile_type
                        ))
                except RuntimeError:
                    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°ã—ãä½œæˆ
                    asyncio.run(self._build_or_update_pecking_order(
                        agent_state, understanding_result, is_retry, task_profile_type
                    ))
            except Exception as pecking_error:
                rich_ui.print_warning(f"The Pecking Order æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {pecking_error}")
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            agent_state.update_duck_vitals(
                confidence_score=classification_result.confidence,
                is_progress=True
            )
            
            # å®Ÿè¡Œçµæœã‚’çŠ¶æ…‹ã«ä¿å­˜
            execution_results = state.get("execution_results", {})
            execution_results["understanding_result"] = understanding_result
            execution_results["task_profile_type"] = task_profile_type
            execution_results["routing_decision"] = routing_decision
            
            return {
                **state,
                "agent_state": agent_state,
                "execution_results": execution_results,
                "current_node": "planning"
            }
            
        except Exception as e:
            rich_ui.print_error(f"è¨ˆç”»ãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            agent_state.update_duck_vitals(had_error=True, is_progress=False)
            return {**state, "agent_state": agent_state, "error": str(e)}
    
    def _information_collection_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """2ï¸âƒ£ æƒ…å ±åé›†ãƒãƒ¼ãƒ‰ (The Librarian) - LangGraphç‰ˆ"""
        try:
            rich_ui.print_step("[The Librarian] æƒ…å ±åé›†ãƒ•ã‚§ãƒ¼ã‚º")
            
            agent_state = state["agent_state"]
            execution_results = state.get("execution_results", {})
            understanding_result = execution_results.get("understanding_result")
            
            if not understanding_result:
                rich_ui.print_warning("ç†è§£çµæœãŒã‚ã‚Šã¾ã›ã‚“ - æœ€å°é™ã®æƒ…å ±åé›†ã‚’å®Ÿè¡Œ")
                return {**state, "current_node": "information_collection"}
            
            # Duck Scanã‚’ä½¿ç”¨ã—ãŸæ¢ç´¢ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆï¼‰
            routing_decision = execution_results.get("routing_decision", {})
            target_files = routing_decision.get("target_files", [])
            
            if target_files:
                # ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«åã§æ¤œç´¢
                primary_file = target_files[0]  # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸»è¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã™ã‚‹
                scan_result = duck_scan.scan_workspace(primary_file)
                rich_ui.print_message(f"[INFO_COLLECTION] ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢: {primary_file}", "info")
            else:
                # ä¸€èˆ¬çš„ãªæ¤œç´¢
                user_message = state["user_message"]
                scan_result = duck_scan.scan_workspace(user_message)
                rich_ui.print_message(f"[INFO_COLLECTION] ä¸€èˆ¬æ¤œç´¢: {user_message}", "info")
            
            rich_ui.print_message(f"Duck Scançµæœ: {len(scan_result.files)}ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹", "info")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±åé›† (Duck FSä½¿ç”¨ + FileContentå¤‰æ›)
            collected_files = {}
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸã‹ãƒã‚§ãƒƒã‚¯
            target_found = False
            if target_files:
                for target_file in target_files:
                    for scanned_file in scan_result.files:
                        if target_file.lower() in scanned_file.lower():
                            target_found = True
                            break
                    if target_found:
                        break
            
            if target_files and not target_found:
                rich_ui.print_warning(f"[INFO_COLLECTION] ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{target_files[0]}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šã‚’è©¦è¡Œ
                for target_file in target_files:
                    try:
                        file_result = duck_fs.read(target_file)
                        from ..prompts.four_node_context import FileContent
                        file_content = FileContent(
                            path=file_result.path,
                            content=file_result.content,
                            encoding=file_result.encoding,
                            size=len(file_result.content),
                            last_modified=datetime.now(),
                            relevance_score=1.0  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€é«˜é–¢é€£åº¦
                        )
                        collected_files[target_file] = file_content
                        rich_ui.print_success(f"[INFO_COLLECTION] ç›´æ¥èª­ã¿å–ã‚ŠæˆåŠŸ: {target_file} ({len(file_result.content)}æ–‡å­—)")
                        target_found = True
                    except Exception as e:
                        rich_ui.print_warning(f"[INFO_COLLECTION] ç›´æ¥èª­ã¿å–ã‚Šå¤±æ•— {target_file}: {e}")
            
            # ã‚¹ã‚­ãƒ£ãƒ³çµæœã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Šï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆå„ªå…ˆï¼‰
            files_to_read = scan_result.files[:10] if not target_found else scan_result.files[:5]
            
            for file_path in files_to_read:
                try:
                    file_result = duck_fs.read(file_path)
                    
                    # Duck FSã®çµæœã‚’FileContentã«å¤‰æ›
                    from ..prompts.four_node_context import FileContent
                    
                    # é–¢é€£åº¦ã®è¨ˆç®—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«è¿‘ã„ã»ã©é«˜ã„ï¼‰
                    relevance_score = 0.5
                    if target_files:
                        for target_file in target_files:
                            if target_file.lower() in file_path.lower():
                                relevance_score = 1.0
                                break
                    
                    file_content = FileContent(
                        path=file_result.path,
                        content=file_result.content,
                        encoding=file_result.encoding,
                        size=len(file_result.content),
                        last_modified=datetime.now(),
                        relevance_score=relevance_score
                    )
                    
                    collected_files[file_path] = file_content
                    rich_ui.print_message(f"[INFO_COLLECTION] èª­ã¿å–ã‚Šå®Œäº†: {file_path} ({len(file_result.content)}æ–‡å­—, é–¢é€£åº¦: {relevance_score:.1f})", "info")
                except Exception as e:
                    rich_ui.print_warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            
            # RAGæ¤œç´¢å®Ÿè¡Œ
            rag_results = self.helpers.perform_rag_search(understanding_result, agent_state)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆæ§‹ç¯‰
            project_context = self.helpers.build_project_context(collected_files, agent_state)
            
            # GatheredInfo ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            gathered_info = GatheredInfo(
                collected_files=collected_files,
                rag_results=rag_results or [],
                project_context=project_context,
                confidence_scores={},
                information_gaps=[],
                collection_strategy="duck_scan_integration"
            )
            
            # çŠ¶æ…‹ã¸ã®ä¿å­˜
            agent_state.collected_context["gathered_info"] = gathered_info
            execution_results["gathered_info"] = gathered_info
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            file_count = len(collected_files)
            agent_state.update_duck_vitals(
                is_progress=file_count > 0,
                context_size=sum(len(str(f.content)) for f in collected_files.values())
            )
            
            return {
                **state,
                "agent_state": agent_state,
                "execution_results": execution_results,
                "current_node": "information_collection"
            }
            
        except Exception as e:
            rich_ui.print_error(f"æƒ…å ±åé›†ãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            agent_state.update_duck_vitals(had_error=True, is_progress=False)
            return {**state, "agent_state": agent_state, "error": str(e)}
    
    def _safe_execution_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """3ï¸âƒ£ å®‰å…¨å®Ÿè¡Œãƒãƒ¼ãƒ‰ (The Operator) - LangGraphç‰ˆ"""
        try:
            rich_ui.print_step("[The Operator] å®‰å…¨å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º")
            
            agent_state = state["agent_state"]
            execution_results = state.get("execution_results", {})
            understanding_result = execution_results.get("understanding_result")
            gathered_info = execution_results.get("gathered_info")
            
            if not understanding_result:
                rich_ui.print_warning("ç†è§£çµæœãŒã‚ã‚Šã¾ã›ã‚“ - å®Ÿè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return {**state, "current_node": "safe_execution"}
            
            # ãƒªã‚¹ã‚¯è©•ä¾¡
            risk_assessment = self.helpers.assess_execution_risks(
                understanding_result, gathered_info, agent_state
            )
            
            rich_ui.print_message(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {risk_assessment.overall_risk.value}", "info")
            
            # æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹ (å¿…è¦ã«å¿œã˜ã¦)
            approval_status = self.helpers.handle_approval_process(
                risk_assessment, understanding_result, agent_state
            )
            
            # å®Ÿè¡Œçµæœä½œæˆï¼ˆç¾åœ¨ã¯èª­ã¿å–ã‚Šå°‚ç”¨æ“ä½œã®ã¿ï¼‰
            execution_result = ExecutionResult(
                success=True,
                error_message=None,
                execution_time=0.1,
                tool_results=[],
                risk_assessment=risk_assessment,
                approval_status=approval_status,
                errors=[]
            )
            
            # çŠ¶æ…‹ã¸ã®ä¿å­˜
            agent_state.collected_context["execution_result"] = execution_result
            execution_results["execution_result"] = execution_result
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            agent_state.update_duck_vitals(
                is_progress=execution_result.success,
                had_error=not execution_result.success
            )
            
            return {
                **state,
                "agent_state": agent_state,
                "execution_results": execution_results,
                "current_node": "safe_execution"
            }
            
        except Exception as e:
            rich_ui.print_error(f"å®‰å…¨å®Ÿè¡Œãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            agent_state.update_duck_vitals(had_error=True, is_progress=False)
            return {**state, "agent_state": agent_state, "error": str(e)}
    
    def _evaluation_continuation_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """4ï¸âƒ£ è©•ä¾¡ãƒ»ç¶™ç¶šãƒãƒ¼ãƒ‰ (Quality Gate & Controller) - LangGraphç‰ˆ"""
        try:
            rich_ui.print_step("[Quality Gate & Controller] è©•ä¾¡ãƒ»ç¶™ç¶šãƒ•ã‚§ãƒ¼ã‚º")
            
            agent_state = state["agent_state"]
            execution_results = state.get("execution_results", {})
            understanding_result = execution_results.get("understanding_result")
            gathered_info = execution_results.get("gathered_info")
            execution_result = execution_results.get("execution_result")
            task_profile_type = execution_results.get("task_profile_type")
            
            # D.U.C.K. Vitals System ã®æ›´æ–°
            self._comprehensive_vitals_update(agent_state, understanding_result, gathered_info, execution_result)
            
            # å‹•çš„Duck Pacemakerå®Ÿè¡Œä¸­ç›£è¦–
            current_loop = state.get("loop_count", 0)
            if hasattr(self, 'dynamic_pacemaker'):
                update_result = self.dynamic_pacemaker.update_during_execution(
                    state=agent_state,
                    current_loop=current_loop
                )
                
                # ä»‹å…¥ãŒå¿…è¦ãªå ´åˆã®å‡¦ç†
                if update_result.get("intervention_required"):
                    intervention_details = update_result.get("intervention_details", {})
                    
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›¸è«‡ã‚’å®Ÿè¡Œ
                    consultation_result = self.user_consultation.present_consultation(
                        state=agent_state,
                        intervention_details=intervention_details,
                        current_loop=current_loop
                    )
                    
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠã‚’å‡¦ç†
                    choice_result = self.user_consultation.process_user_choice(
                        consultation_result, agent_state
                    )
                    
                    # é¸æŠã«åŸºã¥ãæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
                    next_action = choice_result.get("next_action", "continue")
                    
                    # çŠ¶æ…‹æ›´æ–°
                    execution_results["consultation_result"] = consultation_result
                    execution_results["choice_result"] = choice_result
                    execution_results["next_action"] = next_action
                    
                    return {
                        **state,
                        "agent_state": agent_state,
                        "execution_results": execution_results,
                        "current_node": "evaluation_continuation"
                    }
            
            # ã€ä¿®æ­£ã€‘The Pecking Order é€²æ—æ›´æ–° (åŒæœŸç‰ˆ)
            try:
                # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # æ—¢ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå‹•ä½œä¸­ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        rich_ui.print_warning("The Pecking Order é€²æ—: ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å‹•ä½œä¸­ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    else:
                        loop.run_until_complete(self._update_pecking_order_progress(agent_state, execution_result))
                except RuntimeError:
                    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°ã—ãä½œæˆ
                    asyncio.run(self._update_pecking_order_progress(agent_state, execution_result))
            except Exception as pecking_error:
                rich_ui.print_warning(f"The Pecking Order é€²æ—æ›´æ–°ã‚¨ãƒ©ãƒ¼: {pecking_error}")
            
            # å“è³ªè©•ä¾¡
            evaluation_result = self._perform_quality_evaluation(
                understanding_result, gathered_info, execution_result, task_profile_type
            )
            
            # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
            next_action = self._determine_next_action_langgraph(
                agent_state, evaluation_result, understanding_result, gathered_info, execution_result
            )
            
            rich_ui.print_message(f"è©•ä¾¡çµæœ -> æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {next_action}", "info")
            
            # çŠ¶æ…‹æ›´æ–°
            execution_results["evaluation_result"] = evaluation_result
            execution_results["next_action"] = next_action
            
            return {
                **state,
                "agent_state": agent_state,
                "execution_results": execution_results,
                "current_node": "evaluation_continuation"
            }
            
        except Exception as e:
            rich_ui.print_error(f"è©•ä¾¡ãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            agent_state.update_duck_vitals(had_error=True, is_progress=False)
            return {**state, "agent_state": agent_state, "error": str(e)}
    
    def _response_generation_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """5ï¸âƒ£ å¿œç­”ç”Ÿæˆãƒãƒ¼ãƒ‰ (The Scribe) - LangGraphç‰ˆ"""
        try:
            rich_ui.print_step("[The Scribe] å¿œç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º")
            
            agent_state = state["agent_state"]
            execution_results = state.get("execution_results", {})
            gathered_info = execution_results.get("gathered_info")
            execution_result = execution_results.get("execution_result")
            task_profile_type = execution_results.get("task_profile_type", TaskProfileType.GENERAL_CHAT)
            
            # Response Generation Node ã‚’ä½¿ç”¨ã—ã¦æ±ºå®šè«–çš„å¿œç­”ç”Ÿæˆ
            response_result = response_generation_node.generate_response(
                agent_state, gathered_info, execution_result, task_profile_type
            )
            
            final_response = response_result.final_response
            
            # å¿œç­”ã‚’AgentStateã«è¿½åŠ 
            agent_state.add_message("assistant", final_response)
            
            rich_ui.print_success(f"The Scribe: {len(final_response)}æ–‡å­—ã®å¿œç­”ã‚’ç”Ÿæˆ")
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            agent_state.update_duck_vitals(
                confidence_score=0.9,  # æ±ºå®šè«–çš„ç”Ÿæˆãªã®ã§é«˜ä¿¡é ¼åº¦
                is_progress=True
            )
            
            return {
                **state,
                "agent_state": agent_state,
                "final_response": final_response,
                "current_node": "response_generation"
            }
            
        except Exception as e:
            rich_ui.print_error(f"å¿œç­”ç”Ÿæˆãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            agent_state.update_duck_vitals(had_error=True, is_progress=False)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            fallback_response = self._generate_error_response(str(e))
            agent_state.add_message("assistant", fallback_response)
            
            return {
                **state,
                "agent_state": agent_state,
                "final_response": fallback_response,
                "error": str(e)
            }
    
    # === ãƒãƒ¼ãƒ‰å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ (éåŒæœŸç‰ˆ - å¾Œæ–¹äº’æ›æ€§) ===
    
    async def _execute_planning_node(
        self, 
        state_obj: AgentState, 
        user_message: str
    ) -> Tuple[Optional[UnderstandingResult], Optional[TaskProfileType]]:
        """1ï¸âƒ£ ç†è§£ãƒ»è¨ˆç”»ãƒãƒ¼ãƒ‰ (The Architect) ã®å®Ÿè¡Œ"""
        try:
            rich_ui.print_step("[The Architect] ç†è§£ãƒ»è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º")
            
            # è»½é‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
            self.helpers.prepare_lightweight_context(state_obj)
            
            # æ„å›³åˆ†æ (RoutingEngine)
            routing_decision = self.helpers.analyze_user_intent(state_obj)
            
            # TaskProfileåˆ†é¡
            classification_result = task_classifier.classify(user_message)
            task_profile_type = classification_result.profile_type
            
            rich_ui.print_message(f"TaskProfile: {task_profile_type.value} (ä¿¡é ¼åº¦: {classification_result.confidence:.2f})", "info")
            
            # å†è©¦è¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®š
            is_retry = self.helpers.is_retry_context(state_obj)
            
            # Four Node Context ä½œæˆ (æ—¢å­˜ã®ä»•çµ„ã¿ã‚’æ´»ç”¨)
            four_node_context = self._create_four_node_context(state_obj, routing_decision, task_profile_type)
            
            # ç†è§£ãƒ»è¨ˆç”»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ
            understanding_result = self.helpers.execute_understanding_prompt(
                state_obj, four_node_context, routing_decision, is_retry
            )
            
            # ã€è¿½åŠ ã€‘The Pecking Order ã®æ§‹ç¯‰ãƒ»æ›´æ–°
            await self._build_or_update_pecking_order(state_obj, understanding_result, is_retry, task_profile_type)
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            state_obj.update_duck_vitals(
                confidence_score=classification_result.confidence,
                is_progress=True
            )
            
            return understanding_result, task_profile_type
            
        except Exception as e:
            rich_ui.print_error(f"è¨ˆç”»ãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            state_obj.update_duck_vitals(had_error=True, is_progress=False)
            return None, None
    
    async def _execute_information_collection_node(
        self, 
        state_obj: AgentState, 
        understanding_result: Optional[UnderstandingResult]
    ) -> Optional[GatheredInfo]:
        """2ï¸âƒ£ æƒ…å ±åé›†ãƒãƒ¼ãƒ‰ (The Librarian) ã®å®Ÿè¡Œ"""
        try:
            rich_ui.print_step("[The Librarian] æƒ…å ±åé›†ãƒ•ã‚§ãƒ¼ã‚º")
            
            if not understanding_result:
                rich_ui.print_warning("ç†è§£çµæœãŒã‚ã‚Šã¾ã›ã‚“ - æœ€å°é™ã®æƒ…å ±åé›†ã‚’å®Ÿè¡Œ")
                return None
            
            # åé›†æˆ¦ç•¥ã®æ±ºå®š
            collection_strategy = self.helpers.determine_collection_strategy(understanding_result)
            rich_ui.print_message(f"åé›†æˆ¦ç•¥: {collection_strategy}", "info")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±åé›† (æ©Ÿæ¢°çš„å‡¦ç†)
            collected_files = self.helpers.collect_file_information(understanding_result, state_obj)
            
            # RAGæ¤œç´¢å®Ÿè¡Œ
            rag_results = self.helpers.perform_rag_search(understanding_result, state_obj)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆæ§‹ç¯‰
            project_context = self.helpers.build_project_context(collected_files, state_obj)
            
            # GatheredInfo ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            gathered_info = GatheredInfo(
                collected_files=collected_files,
                rag_results=rag_results,
                project_context=project_context,
                collection_strategy=collection_strategy,
                collection_timestamp=datetime.now()
            )
            
            # çŠ¶æ…‹ã¸ã®ä¿å­˜
            state_obj.collected_context["gathered_info"] = gathered_info
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            file_count = len(collected_files) if collected_files else 0
            state_obj.update_duck_vitals(
                is_progress=file_count > 0,
                context_size=sum(len(str(f)) for f in collected_files.values()) if collected_files else 0
            )
            
            return gathered_info
            
        except Exception as e:
            rich_ui.print_error(f"æƒ…å ±åé›†ãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            state_obj.update_duck_vitals(had_error=True, is_progress=False)
            return None
    
    async def _execute_safe_execution_node(
        self, 
        state_obj: AgentState, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo]
    ) -> Optional[ExecutionResult]:
        """3ï¸âƒ£ å®‰å…¨å®Ÿè¡Œãƒãƒ¼ãƒ‰ (The Operator) ã®å®Ÿè¡Œ"""
        try:
            rich_ui.print_step("[The Operator] å®‰å…¨å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º")
            
            if not understanding_result:
                rich_ui.print_warning("ç†è§£çµæœãŒã‚ã‚Šã¾ã›ã‚“ - å®Ÿè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return None
            
            # ãƒªã‚¹ã‚¯è©•ä¾¡
            risk_assessment = self.helpers.assess_execution_risks(
                understanding_result, gathered_info, state_obj
            )
            
            rich_ui.print_message(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {risk_assessment.overall_risk.value}", "info")
            
            # æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹ (å¿…è¦ã«å¿œã˜ã¦)
            approval_status = self.helpers.handle_approval_process(
                risk_assessment, understanding_result, state_obj
            )
            
            if approval_status.requested and not approval_status.granted:
                rich_ui.print_warning("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿè¡Œã‚’æ‹’å¦ã—ã¾ã—ãŸ")
                return ExecutionResult(
                    success=False,
                    error_message="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹å®Ÿè¡Œæ‹’å¦",
                    execution_time=0.0,
                    tool_results=[],
                    risk_assessment=risk_assessment,
                    approval_status=approval_status,
                    errors=[]
                )
            
            # å®Ÿéš›ã®å®Ÿè¡Œã¯èª­ã¿å–ã‚Šå°‚ç”¨æ“ä½œã®ã¿ (æ›¸ãè¾¼ã¿ç³»ã¯å°†æ¥å®Ÿè£…)
            execution_result = ExecutionResult(
                success=True,
                error_message=None,
                execution_time=0.1,
                tool_results=[],
                risk_assessment=risk_assessment,
                approval_status=approval_status,
                errors=[]
            )
            
            # çŠ¶æ…‹ã¸ã®ä¿å­˜
            state_obj.collected_context["execution_result"] = execution_result
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            state_obj.update_duck_vitals(
                is_progress=execution_result.success,
                had_error=not execution_result.success
            )
            
            return execution_result
            
        except Exception as e:
            rich_ui.print_error(f"å®‰å…¨å®Ÿè¡Œãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            state_obj.update_duck_vitals(had_error=True, is_progress=False)
            return None
    
    async def _execute_evaluation_node(
        self, 
        state_obj: AgentState, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> Tuple[Optional[EvaluationResult], NodeType]:
        """4ï¸âƒ£ è©•ä¾¡ãƒ»ç¶™ç¶šãƒãƒ¼ãƒ‰ (Quality Gate & Controller) ã®å®Ÿè¡Œ"""
        try:
            rich_ui.print_step("[Quality Gate & Controller] è©•ä¾¡ãƒ»ç¶™ç¶šãƒ•ã‚§ãƒ¼ã‚º")
            
            # D.U.C.K. Vitals System ã®æ›´æ–°
            self._comprehensive_vitals_update(state_obj, understanding_result, gathered_info, execution_result)
            
            # ã€è¿½åŠ ã€‘The Pecking Order çŠ¶æ…‹æ›´æ–°
            await self._update_pecking_order_progress(state_obj, execution_result)
            
            # LLMã‚’ä½¿ç”¨ã—ãŸå“è³ªè©•ä¾¡
            evaluation_result = await self._perform_llm_evaluation(
                state_obj, understanding_result, gathered_info, execution_result, task_profile_type
            )
            
            # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
            next_node = self._determine_next_action(
                state_obj, evaluation_result, understanding_result, gathered_info, execution_result
            )
            
            rich_ui.print_message(f"è©•ä¾¡çµæœ -> æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {next_node}", "info")
            
            # ãƒã‚¤ã‚¿ãƒ«ãƒ™ãƒ¼ã‚¹ã®å¼·åˆ¶ä»‹å…¥ãƒã‚§ãƒƒã‚¯ (Duck Pacemaker)
            intervention = state_obj.needs_duck_intervention()
            if intervention["required"]:
                if intervention["priority"] == "CRITICAL":
                    return evaluation_result, "DUCK_CALL"
                elif intervention["priority"] == "HIGH":
                    return evaluation_result, NodeType.PLANNING  # å†è¨ˆç”»å¼·åˆ¶
            
            return evaluation_result, next_node
            
        except Exception as e:
            rich_ui.print_error(f"è©•ä¾¡ãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            state_obj.update_duck_vitals(had_error=True, is_progress=False)
            return None, NodeType.RESPONSE_GENERATION  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    async def _execute_response_generation_node(
        self, 
        state_obj: AgentState, 
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: TaskProfileType
    ) -> str:
        """5ï¸âƒ£ å¿œç­”ç”Ÿæˆãƒãƒ¼ãƒ‰ (The Scribe) ã®å®Ÿè¡Œ"""
        try:
            rich_ui.print_step("[The Scribe] å¿œç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º")
            
            # Response Generation Node ã‚’ä½¿ç”¨ã—ã¦æ±ºå®šè«–çš„å¿œç­”ç”Ÿæˆ
            response_result = response_generation_node.generate_response(
                state_obj, gathered_info, execution_result, task_profile_type
            )
            
            rich_ui.print_success(f"The Scribe: {len(response_result.final_response)}æ–‡å­—ã®å¿œç­”ã‚’ç”Ÿæˆ")
            
            # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
            state_obj.update_duck_vitals(
                confidence_score=0.9,  # æ±ºå®šè«–çš„ç”Ÿæˆãªã®ã§é«˜ä¿¡é ¼åº¦
                is_progress=True
            )
            
            return response_result.final_response
            
        except Exception as e:
            rich_ui.print_error(f"å¿œç­”ç”Ÿæˆãƒãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            state_obj.update_duck_vitals(had_error=True, is_progress=False)
            return self._generate_error_response(str(e))
    
    # === LangGraphæ¡ä»¶åˆ†å²ãƒ¡ã‚½ãƒƒãƒ‰ ===
    
    def _after_planning(self, state: Dict[str, Any]) -> str:
        """ç†è§£ãƒ»è¨ˆç”»ãƒãƒ¼ãƒ‰å¾Œã®åˆ†å²åˆ¤å®š"""
        try:
            execution_results = state.get("execution_results", {})
            understanding_result = execution_results.get("understanding_result")
            routing_decision = execution_results.get("routing_decision", {})
            
            if not understanding_result:
                return "end"
            
            rich_ui.print_message(f"[PLANNING_BRANCH] åˆ†å²åˆ¤å®šé–‹å§‹", "info")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚ŠãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆRoutingEngineã®çµæœã‚’ä½¿ç”¨ï¼‰
            needs_file_read = routing_decision.get("needs_file_read", False)
            target_files = routing_decision.get("target_files", [])
            
            rich_ui.print_message(f"[PLANNING_BRANCH] needs_file_read: {needs_file_read}, target_files: {len(target_files)}", "info")
            
            if needs_file_read and target_files:
                rich_ui.print_message("[PLANNING_BRANCH] â†’ æƒ…å ±åé›†ãƒãƒ¼ãƒ‰ã¸", "info")
                return "information_collection"
            
            # æƒ…å ±åé›†ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
            if hasattr(understanding_result, 'information_needs') and understanding_result.information_needs:
                rich_ui.print_message("[PLANNING_BRANCH] â†’ æƒ…å ±åé›†ãƒãƒ¼ãƒ‰ã¸ï¼ˆinformation_needsï¼‰", "info")
                return "information_collection"
            
            # å®Ÿè¡Œè¨ˆç”»ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if understanding_result.execution_plan and understanding_result.execution_plan.required_tools:
                rich_ui.print_message("[PLANNING_BRANCH] â†’ å®‰å…¨å®Ÿè¡Œãƒãƒ¼ãƒ‰ã¸", "info")
                return "safe_execution"
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å¿œç­”ç”Ÿæˆ
            rich_ui.print_message("[PLANNING_BRANCH] â†’ å¿œç­”ç”Ÿæˆãƒãƒ¼ãƒ‰ã¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰", "info")
            return "response_generation"
            
        except Exception as e:
            rich_ui.print_error(f"è¨ˆç”»å¾Œåˆ†å²åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return "end"
    
    def _after_information_collection(self, state: Dict[str, Any]) -> str:
        """æƒ…å ±åé›†ãƒãƒ¼ãƒ‰å¾Œã®åˆ†å²åˆ¤å®š"""
        try:
            execution_results = state.get("execution_results", {})
            gathered_info = execution_results.get("gathered_info")
            understanding_result = execution_results.get("understanding_result")
            
            if not gathered_info:
                return "end"
            
            # å®Ÿè¡ŒãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
            if understanding_result and understanding_result.execution_plan:
                required_tools = understanding_result.execution_plan.required_tools
                if required_tools and any(tool not in ['read_file', 'list_files'] for tool in required_tools):
                    return "safe_execution"
            
            # æƒ…å ±åé›†ã®ã¿ã§å®Œäº†ã®å ´åˆã¯è©•ä¾¡ã¸
            return "evaluation_continuation"
            
        except Exception as e:
            rich_ui.print_error(f"æƒ…å ±åé›†å¾Œåˆ†å²åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return "end"
    
    def _after_evaluation_continuation(self, state: Dict[str, Any]) -> str:
        """è©•ä¾¡ãƒ»ç¶™ç¶šãƒãƒ¼ãƒ‰å¾Œã®åˆ†å²åˆ¤å®š"""
        try:
            execution_results = state.get("execution_results", {})
            next_action = execution_results.get("next_action")
            loop_count = state.get("loop_count", 0)
            
            # ãƒ«ãƒ¼ãƒ—åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if loop_count >= 10:
                rich_ui.print_warning("æœ€å¤§ãƒ«ãƒ¼ãƒ—å›æ•°ã«åˆ°é”")
                return "response_generation"
            
            # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«åŸºã¥ãåˆ†å²
            if next_action == "planning":
                return "planning"
            elif next_action == "information_collection":
                return "information_collection"
            elif next_action == "safe_execution":
                return "safe_execution"
            elif next_action == "response_generation":
                return "response_generation"
            else:
                return "end"
                
        except Exception as e:
            rich_ui.print_error(f"è©•ä¾¡å¾Œåˆ†å²åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return "end"
    
    def _after_response_generation(self, state: Dict[str, Any]) -> str:
        """å¿œç­”ç”Ÿæˆãƒãƒ¼ãƒ‰å¾Œã®åˆ†å²åˆ¤å®š"""
        try:
            # å¿œç­”ç”Ÿæˆå¾Œã¯å“è³ªãƒã‚§ãƒƒã‚¯ã®ãŸã‚è©•ä¾¡ãƒãƒ¼ãƒ‰ã«æˆ»ã‚‹
            final_response = state.get("final_response")
            
            if final_response and len(final_response) > 100:
                # ååˆ†ãªå¿œç­”ãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆã¯çµ‚äº†
                return "end"
            else:
                # ä¸ååˆ†ãªå ´åˆã¯è©•ä¾¡ãƒãƒ¼ãƒ‰ã§å†æ¤œè¨
                return "evaluation_continuation"
                
        except Exception as e:
            rich_ui.print_error(f"å¿œç­”ç”Ÿæˆå¾Œåˆ†å²åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return "end"
    
    # === ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ===
    
    def _initialize_session(self, state_obj: AgentState, user_message: str) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
        self.loop_count = 0
        self.current_node = None
        self.node_execution_history = []
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
        state_obj.add_message("user", user_message)
        
        # åˆæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
        if not hasattr(state_obj, 'collected_context'):
            state_obj.collected_context = {}
    
    def _create_four_node_context(
        self, 
        state_obj: AgentState, 
        routing_decision: Dict[str, Any], 
        task_profile_type: TaskProfileType
    ):
        """Four Node Context ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ (æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®äº’æ›æ€§)"""
        from ..prompts.four_node_context import FourNodePromptContext, NodeType
        from pathlib import Path
        
        # å¿…é ˆå¼•æ•°ã‚’æŒ‡å®šã—ã¦FourNodePromptContextã‚’ä½œæˆ
        context = FourNodePromptContext(
            current_node=NodeType.UNDERSTANDING,
            execution_phase=1,
            workspace_path=Path.cwd()
        )
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å±æ€§ã‚’è¨­å®š
        context.operation_type = routing_decision.get("operation_type", "chat")
        
        # ã€ä¿®æ­£ã€‘The Pecking Orderæƒ…å ±ã‚’çµ±åˆ
        current_task = state_obj.get_current_task()
        if current_task:
            context.current_task = current_task.description
            # éšå±¤çš„ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            context.pecking_order_status = state_obj.get_pecking_order_status()
            context.task_hierarchy = state_obj.get_pecking_order_string()
        else:
            context.current_task = f"TaskProfile: {task_profile_type.value}"
        
        return context
    
    def _update_duck_vitals(self, state_obj: AgentState, current_node: NodeType) -> None:
        """ãƒãƒ¼ãƒ‰å®Ÿè¡Œæ™‚ã®ãƒã‚¤ã‚¿ãƒ«æ›´æ–°"""
        # å„ãƒãƒ¼ãƒ‰ã§ã®æ¨™æº–çš„ãªãƒã‚¤ã‚¿ãƒ«æ›´æ–°
        confidence_map = {
            NodeType.PLANNING: 0.8,
            NodeType.INFORMATION_COLLECTION: 0.9,
            NodeType.SAFE_EXECUTION: 0.7,
            NodeType.EVALUATION_CONTINUATION: 0.8,
            NodeType.RESPONSE_GENERATION: 0.9
        }
        
        confidence = confidence_map.get(current_node, 0.7)
        state_obj.update_duck_vitals(
            confidence_score=confidence,
            is_progress=True
        )
    
    def _comprehensive_vitals_update(
        self, 
        state_obj: AgentState, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult]
    ) -> None:
        """åŒ…æ‹¬çš„ãªãƒã‚¤ã‚¿ãƒ«æ›´æ–° (è©•ä¾¡ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ)"""
        # æˆåŠŸç‡ã«åŸºã¥ãä¿¡é ¼åº¦è¨ˆç®—
        success_indicators = 0
        total_indicators = 3
        
        if understanding_result:
            success_indicators += 1
        if gathered_info and gathered_info.collected_files:
            success_indicators += 1
        if execution_result and execution_result.success:
            success_indicators += 1
        
        confidence_score = success_indicators / total_indicators
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºè¨ˆç®—
        context_size = 0
        if gathered_info and gathered_info.collected_files:
            for file_content in gathered_info.collected_files.values():
                if hasattr(file_content, 'content'):
                    context_size += len(file_content.content)
        
        # ã‚¨ãƒ©ãƒ¼æœ‰ç„¡ãƒã‚§ãƒƒã‚¯
        had_error = (execution_result and not execution_result.success) or confidence_score < 0.3
        
        # ãƒã‚¤ã‚¿ãƒ«æ›´æ–°
        state_obj.update_duck_vitals(
            confidence_score=confidence_score,
            had_error=had_error,
            is_progress=success_indicators > 0,
            context_size=context_size
        )
    
    async def _perform_llm_evaluation(
        self, 
        state_obj: AgentState, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> Optional[EvaluationResult]:
        """LLMã‚’ä½¿ç”¨ã—ãŸå“è³ªè©•ä¾¡"""
        try:
            # è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            evaluation_prompt = self._build_evaluation_prompt(
                understanding_result, gathered_info, execution_result, task_profile_type
            )
            
            # LLMè©•ä¾¡å®Ÿè¡Œ
            response = llm_manager.chat(evaluation_prompt)
            
            # è©•ä¾¡çµæœè§£æ
            evaluation_result = self._parse_evaluation_response(response)
            
            return evaluation_result
            
        except Exception as e:
            rich_ui.print_warning(f"LLMè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _build_evaluation_prompt(
        self, 
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> str:
        """è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        prompt_parts = [
            "ä»¥ä¸‹ã®æƒ…å ±ã‚’è©•ä¾¡ã—ã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚",
            "",
            f"TaskProfile: {task_profile_type.value if task_profile_type else 'Unknown'}",
            "",
            "å®Ÿè¡ŒçŠ¶æ³:",
        ]
        
        if understanding_result:
            prompt_parts.append(f"âœ… ç†è§£ãƒ»è¨ˆç”»: å®Œäº† (ä¿¡é ¼åº¦: {understanding_result.confidence:.2f})")
        else:
            prompt_parts.append("âŒ ç†è§£ãƒ»è¨ˆç”»: æœªå®Œäº†")
        
        if gathered_info:
            file_count = len(gathered_info.collected_files) if gathered_info.collected_files else 0
            prompt_parts.append(f"âœ… æƒ…å ±åé›†: å®Œäº† ({file_count}ãƒ•ã‚¡ã‚¤ãƒ«)")
        else:
            prompt_parts.append("âŒ æƒ…å ±åé›†: æœªå®Œäº†")
        
        if execution_result:
            status = "æˆåŠŸ" if execution_result.success else "å¤±æ•—"
            prompt_parts.append(f"{'âœ…' if execution_result.success else 'âŒ'} å®Ÿè¡Œ: {status}")
        else:
            prompt_parts.append("âŒ å®Ÿè¡Œ: æœªå®Ÿè¡Œ")
        
        prompt_parts.extend([
            "",
            "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            "1. RESPONSE_GENERATION - å¿œç­”ç”Ÿæˆã¸é€²ã‚€",
            "2. REPLAN - å†è¨ˆç”»ãŒå¿…è¦",
            "3. COLLECT_MORE_INFO - è¿½åŠ æƒ…å ±åé›†ãŒå¿…è¦", 
            "4. EXECUTE_ADDITIONAL - è¿½åŠ å®Ÿè¡ŒãŒå¿…è¦",
            "5. END - ã‚¿ã‚¹ã‚¯å®Œäº†",
            "",
            "é¸æŠç†ç”±ã¨å…±ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        ])
        
        return "\n".join(prompt_parts)
    
    def _parse_evaluation_response(self, response: str) -> EvaluationResult:
        """è©•ä¾¡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ"""
        # ç°¡æ˜“è§£æ (å°†æ¥çš„ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªè§£æã‚’å®Ÿè£…)
        next_action = NextAction.RESPONSE_GENERATION  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        response_lower = response.lower()
        if "replan" in response_lower or "å†è¨ˆç”»" in response_lower:
            next_action = NextAction.REPLAN
        elif "collect_more" in response_lower or "è¿½åŠ æƒ…å ±" in response_lower:
            next_action = NextAction.COLLECT_MORE_INFO
        elif "execute_additional" in response_lower or "è¿½åŠ å®Ÿè¡Œ" in response_lower:
            next_action = NextAction.EXECUTE_ADDITIONAL
        elif "end" in response_lower or "å®Œäº†" in response_lower:
            next_action = NextAction.END
        
        return EvaluationResult(
            overall_quality_score=0.8,  # ç°¡æ˜“å®Ÿè£…
            task_completion_status="in_progress",
            identified_issues=[],
            recommended_next_action=next_action,
            confidence_in_recommendation=0.8,
            reasoning=response[:200],  # æœ€åˆã®200æ–‡å­—
            duck_vitals_assessment={"mood": 0.8, "focus": 0.7, "stamina": 0.8}
        )
    
    def _determine_next_action(
        self, 
        state_obj: AgentState, 
        evaluation_result: Optional[EvaluationResult],
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult]
    ) -> NodeType:
        """æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š"""
        # LLMè©•ä¾¡çµæœãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
        if evaluation_result:
            action_map = {
                NextAction.RESPONSE_GENERATION: NodeType.RESPONSE_GENERATION,
                NextAction.REPLAN: NodeType.PLANNING,
                NextAction.COLLECT_MORE_INFO: NodeType.INFORMATION_COLLECTION,
                NextAction.EXECUTE_ADDITIONAL: NodeType.SAFE_EXECUTION,
                NextAction.END: "END"
            }
            
            next_action = action_map.get(evaluation_result.recommended_next_action, NodeType.RESPONSE_GENERATION)
            if next_action:
                return next_action
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ±ºå®š
        if not understanding_result:
            return NodeType.PLANNING
        elif not gathered_info:
            return NodeType.INFORMATION_COLLECTION
        elif not execution_result:
            return NodeType.SAFE_EXECUTION
        else:
            return NodeType.RESPONSE_GENERATION
    
    def _record_node_execution(self, node_type: NodeType, metadata: Dict[str, Any]) -> None:
        """ãƒãƒ¼ãƒ‰å®Ÿè¡Œå±¥æ­´ã®è¨˜éŒ²"""
        self.node_execution_history.append({
            "node_type": node_type.value,
            "timestamp": datetime.now(),
            "metadata": metadata
        })
    
    def _generate_consultation_response(self, state_obj: AgentState, intervention: Dict[str, Any]) -> str:
        """Duck Pacemaker ç›¸è«‡å¿œç­”ç”Ÿæˆ"""
        vitals_display = state_obj.get_duck_status_display()
        
        return f"""# ğŸ¦† Duck Pacemaker ã‹ã‚‰ã®ç›¸è«‡

{intervention['reason']}ã®ãŸã‚ã€ä¸€æ™‚åœæ­¢ã—ã¦ã„ã¾ã™ã€‚

## ç¾åœ¨ã®çŠ¶æ…‹
{vitals_display}

## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
{intervention['action']}

## å¯¾å‡¦æ–¹æ³•
1. ã—ã°ã‚‰ãä¼‘æ†©ã‚’å–ã£ã¦ãã ã•ã„
2. ã‚¿ã‚¹ã‚¯ã‚’å°ã•ãåˆ†å‰²ã—ã¦ã¿ã¦ãã ã•ã„  
3. ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤ºã‚’æä¾›ã—ã¦ãã ã•ã„
4. å¿…è¦ã«å¿œã˜ã¦äººé–“ã®ã‚µãƒãƒ¼ãƒˆã‚’æ±‚ã‚ã¦ãã ã•ã„

---
*Duck Pacemaker ã«ã‚ˆã‚‹è‡ªå‹•ä»‹å…¥*"""
    
    def _generate_duck_call_response(self, state_obj: AgentState) -> str:
        """Duck Call å¿œç­”ç”Ÿæˆ"""
        return """# ğŸ¦† Duck Call - äººé–“ã¸ã®ç›¸è«‡

ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã¯è¤‡é›‘ã™ãã‚‹ã‹ã€è¿½åŠ ã®åˆ¤æ–­ãŒå¿…è¦ã§ã™ã€‚

## çŠ¶æ³
- è‡ªå‹•å‡¦ç†ã®é™ç•Œã«é”ã—ã¾ã—ãŸ
- äººé–“ã®åˆ¤æ–­ãŒå¿…è¦ã§ã™

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. ç¾åœ¨ã®é€²æ—ã‚’ç¢ºèªã—ã¦ãã ã•ã„
2. è¿½åŠ ã®æŒ‡ç¤ºã‚„ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„
3. ã‚¿ã‚¹ã‚¯ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„

ãŠæ‰‹æ•°ã§ã™ãŒã€è¿½åŠ ã®ã‚µãƒãƒ¼ãƒˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

---
*Duck Call ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹è‡ªå‹•è¦è«‹*"""
    
    def _generate_timeout_response(self, state_obj: AgentState) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¿œç­”ç”Ÿæˆ"""
        return f"""# â° å‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

æœ€å¤§ãƒ«ãƒ¼ãƒ—å›æ•° ({self.max_loops}) ã«åˆ°é”ã—ã¾ã—ãŸã€‚

## å®Ÿè¡Œå±¥æ­´
{len(self.node_execution_history)}å€‹ã®ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚

## æ¨å¥¨äº‹é …
1. ã‚¿ã‚¹ã‚¯ã‚’ã‚ˆã‚Šå°ã•ãåˆ†å‰²ã—ã¦ãã ã•ã„
2. ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤ºã‚’æä¾›ã—ã¦ãã ã•ã„
3. å¿…è¦ã«å¿œã˜ã¦æ®µéšçš„ã«é€²ã‚ã¦ãã ã•ã„

ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨ã®å½¢å¼ã§ã¯ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

---
*5-Node Orchestrator ã«ã‚ˆã‚‹è‡ªå‹•ç”Ÿæˆ*"""
    
    def _perform_quality_evaluation(
        self,
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo], 
        execution_result: Optional[ExecutionResult],
        task_profile_type: Optional[TaskProfileType]
    ) -> EvaluationResult:
        """å“è³ªè©•ä¾¡ã®å®Ÿè¡Œï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            # æˆåŠŸæŒ‡æ¨™ã®è¨ˆç®—
            success_indicators = 0
            total_indicators = 3
            
            if understanding_result:
                success_indicators += 1
            if gathered_info and gathered_info.collected_files:
                success_indicators += 1
            if execution_result and execution_result.success:
                success_indicators += 1
            
            quality_score = success_indicators / total_indicators
            
            # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
            if quality_score >= 0.8:
                next_action = NextAction.RESPONSE_GENERATION
            elif quality_score >= 0.5:
                next_action = NextAction.CONTINUE
            else:
                next_action = NextAction.RETRY
            
            return EvaluationResult(
                overall_quality_score=quality_score,
                task_completion_status="completed" if quality_score >= 0.8 else "in_progress",
                identified_issues=[],
                recommended_next_action=next_action,
                confidence_in_recommendation=0.8,
                reasoning=f"å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.2f}",
                duck_vitals_assessment={"mood": 0.8, "focus": 0.7, "stamina": 0.8}
            )
            
        except Exception as e:
            rich_ui.print_warning(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return EvaluationResult(
                overall_quality_score=0.5,
                task_completion_status="error",
                identified_issues=[str(e)],
                recommended_next_action=NextAction.RESPONSE_GENERATION,
                confidence_in_recommendation=0.3,
                reasoning=f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}",
                duck_vitals_assessment={"mood": 0.5, "focus": 0.5, "stamina": 0.5}
            )
    
    def _determine_next_action_langgraph(
        self,
        state_obj: AgentState,
        evaluation_result: Optional[EvaluationResult],
        understanding_result: Optional[UnderstandingResult],
        gathered_info: Optional[GatheredInfo],
        execution_result: Optional[ExecutionResult]
    ) -> str:
        """LangGraphç”¨ã®æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š"""
        if evaluation_result:
            action_map = {
                NextAction.RESPONSE_GENERATION: "response_generation",
                NextAction.REPLAN: "planning",
                NextAction.COLLECT_MORE_INFO: "information_collection",
                NextAction.EXECUTE_ADDITIONAL: "safe_execution",
                NextAction.END: "end",
                NextAction.CONTINUE: "response_generation"
            }
            
            return action_map.get(evaluation_result.recommended_next_action, "response_generation")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ±ºå®š
        if not understanding_result:
            return "planning"
        elif not gathered_info:
            return "information_collection"
        elif not execution_result:
            return "safe_execution"
        else:
            return "response_generation"
    
    def _update_execution_stats(self, success: bool, execution_time: float) -> None:
        """å®Ÿè¡Œçµ±è¨ˆã®æ›´æ–°"""
        self.execution_stats['total_runs'] += 1
        
        if success:
            self.execution_stats['successful_runs'] += 1
        else:
            self.execution_stats['failed_runs'] += 1
        
        # å¹³å‡å®Ÿè¡Œæ™‚é–“ã®æ›´æ–°
        total_time = self.execution_stats['average_execution_time'] * (self.execution_stats['total_runs'] - 1)
        self.execution_stats['average_execution_time'] = (total_time + execution_time) / self.execution_stats['total_runs']
    
    def _generate_error_response(self, error_message: str) -> str:
        """ã‚¨ãƒ©ãƒ¼å¿œç­”ç”Ÿæˆ"""
        return f"""# âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼

å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

## ã‚¨ãƒ©ãƒ¼è©³ç´°
{error_message}

## å¯¾å‡¦æ–¹æ³•
1. è¦æ±‚ã‚’å†åº¦ç¢ºèªã—ã¦ãã ã•ã„
2. ã‚ˆã‚Šç°¡å˜ãªå†…å®¹ã‹ã‚‰é–‹å§‹ã—ã¦ãã ã•ã„
3. ã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶šã™ã‚‹å ´åˆã¯ç®¡ç†è€…ã«ã”é€£çµ¡ãã ã•ã„

ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

---
*5-Node Orchestrator ã«ã‚ˆã‚‹è‡ªå‹•ç”Ÿæˆ*"""

    # ===== The Pecking Order é–¢é€£ãƒ¡ã‚½ãƒƒãƒ‰ =====
    
    async def _build_or_update_pecking_order(
        self, 
        state_obj: AgentState, 
        understanding_result: Optional[UnderstandingResult], 
        is_continuation: bool,
        task_profile_type: TaskProfileType
    ) -> None:
        """The Pecking Orderï¼ˆéšå±¤çš„ã‚¿ã‚¹ã‚¯ç®¡ç†ï¼‰ã‚’æ§‹ç¯‰ã¾ãŸã¯æ›´æ–°ã™ã‚‹
        
        Args:
            state_obj: AgentState
            understanding_result: UnderstandingResult
            is_continuation: ç¶™ç¶šå®Ÿè¡Œã‹ã©ã†ã‹
            task_profile_type: TaskProfileåˆ†é¡çµæœ
        """
        try:
            # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            latest_user_message = self._get_latest_user_message(state_obj)
            if not latest_user_message:
                return
                
            # LLMServiceå‘¼ã³å‡ºã—ã§ã‚¿ã‚¹ã‚¯æ§‹é€ ã‚’åˆ†æ
            task_structure = await llm_service.analyze_task_hierarchy(
                user_request=latest_user_message,
                context=understanding_result.requirement_analysis if understanding_result else "",
                is_continuation=is_continuation,
                task_profile_type=task_profile_type.value  # TaskProfileã‚’è€ƒæ…®
            )
            
            if not task_structure:
                rich_ui.print_warning("ã‚¿ã‚¹ã‚¯æ§‹é€ ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # æ–°è¦ã¾ãŸã¯æ—¢å­˜ã‚¿ã‚¹ã‚¯ãƒ„ãƒªãƒ¼ã®å‡¦ç†
            if not state_obj.task_tree or not is_continuation:
                # æ–°è¦ã‚¿ã‚¹ã‚¯ãƒ„ãƒªãƒ¼ã®ä½œæˆ
                main_goal = task_structure.get('main_goal', latest_user_message[:100])
                root_description = task_structure.get('root_task', latest_user_message)
                
                root_task = state_obj.initialize_pecking_order(main_goal, root_description)
                rich_ui.print_step(f"ğŸ¦† The Pecking Order åˆæœŸåŒ–: {main_goal}")
                
                # TaskProfileã«åŸºã¥ãã‚µãƒ–ã‚¿ã‚¹ã‚¯ç”Ÿæˆæˆ¦ç•¥
                max_subtasks = self._get_max_subtasks_for_profile(task_profile_type)
                sub_tasks = task_structure.get('sub_tasks', [])
                
                for i, sub_task_desc in enumerate(sub_tasks[:max_subtasks]):
                    sub_task = state_obj.add_sub_task(root_task.id, sub_task_desc, priority=i)
                    if sub_task:
                        # TaskProfileãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                        sub_task.metadata = sub_task.metadata or {}
                        sub_task.metadata['task_profile_type'] = task_profile_type.value
                        rich_ui.print_message(f"  â””â”€ {sub_task_desc[:50]}...", "info")
                
            else:
                # æ—¢å­˜ã‚¿ã‚¹ã‚¯ãƒ„ãƒªãƒ¼ã®æ›´æ–°
                if state_obj.task_tree:
                    # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                    current_task = state_obj.get_current_task()
                    if current_task:
                        rich_ui.print_step(f"ğŸ”„ ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯: {current_task.description}")
                    
                    # æ–°ã—ã„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Œã°è¿½åŠ 
                    new_sub_tasks = task_structure.get('additional_sub_tasks', [])
                    if new_sub_tasks and state_obj.task_tree:
                        for sub_task_desc in new_sub_tasks[:3]:  # æœ€å¤§3å€‹ã¾ã§
                            sub_task = state_obj.add_sub_task(state_obj.task_tree.id, sub_task_desc)
                            if sub_task:
                                sub_task.metadata = sub_task.metadata or {}
                                sub_task.metadata['task_profile_type'] = task_profile_type.value
                                rich_ui.print_message(f"  â• è¿½åŠ : {sub_task_desc[:50]}...", "info")
            
            # The Pecking Order ã®çŠ¶æ…‹è¡¨ç¤º
            if state_obj.task_tree:
                status_summary = state_obj.get_pecking_order_status()
                completion_rate = status_summary.get('completion_rate', 0.0)
                total_tasks = status_summary.get('total_tasks', 0)
                
                rich_ui.print_message(f"ğŸ“‹ ã‚¿ã‚¹ã‚¯éšå±¤: {total_tasks}å€‹ã®ã‚¿ã‚¹ã‚¯ï¼ˆå®Œäº†ç‡: {completion_rate:.1%}ï¼‰", "info")
                
                # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯è©³ç´°è¡¨ç¤º
                if state_obj.debug_mode:
                    rich_ui.print_step("ğŸ› The Pecking Order è©³ç´°:")
                    hierarchy_str = state_obj.get_pecking_order_string()
                    rich_ui.print_message(hierarchy_str, "debug")
            
        except Exception as e:
            rich_ui.print_error(f"The Pecking Order æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ—ãƒ­ã‚»ã‚¹ã¯ç¶šè¡Œ
    
    def _get_max_subtasks_for_profile(self, task_profile_type: TaskProfileType) -> int:
        """TaskProfileã«åŸºã¥ãæœ€å¤§ã‚µãƒ–ã‚¿ã‚¹ã‚¯æ•°ã‚’æ±ºå®šã™ã‚‹"""
        profile_limits = {
            TaskProfileType.CREATION_REQUEST: 7,  # ä½œæˆç³»ã¯å¤šæ®µéš
            TaskProfileType.ANALYSIS_REQUEST: 5,  # åˆ†æç³»ã¯ä¸­ç¨‹åº¦
            TaskProfileType.MODIFICATION_REQUEST: 6,  # ä¿®æ­£ç³»ã¯ä¸­ç¨‹åº¦
            TaskProfileType.GENERAL_CHAT: 3,  # ä¸€èˆ¬ä¼šè©±ã¯å°‘ãªã‚
            TaskProfileType.QUESTION_ANSWER: 4,  # Q&Aã¯ä¸­ç¨‹åº¦
        }
        return profile_limits.get(task_profile_type, 5)
    
    async def _update_current_task_status(
        self, 
        state_obj: AgentState, 
        status: 'TaskStatus', 
        result: Optional[str] = None, 
        error: Optional[str] = None
    ) -> None:
        """ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
        
        Args:
            state_obj: AgentState
            status: æ–°ã—ã„ã‚¿ã‚¹ã‚¯çŠ¶æ…‹
            result: å®Ÿè¡Œçµæœï¼ˆä»»æ„ï¼‰
            error: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆä»»æ„ï¼‰
        """
        try:
            from ..state.pecking_order import TaskStatus
            
            current_task = state_obj.get_current_task()
            if not current_task:
                return
            
            old_status = current_task.status
            current_task.update_status(status, result, error)
            
            # çŠ¶æ…‹å¤‰æ›´ã®é€šçŸ¥
            status_symbols = {
                TaskStatus.PENDING: "â³",
                TaskStatus.IN_PROGRESS: "ğŸ”„",
                TaskStatus.COMPLETED: "âœ…",
                TaskStatus.FAILED: "âŒ"
            }
            
            rich_ui.print_message(
                f"ã‚¿ã‚¹ã‚¯çŠ¶æ…‹æ›´æ–°: {status_symbols[old_status]} â†’ {status_symbols[status]} {current_task.description[:50]}...",
                "info"
            )
            
            # å®Œäº†æ™‚ã¯æ¬¡ã®ã‚¿ã‚¹ã‚¯ã«ç§»è¡Œ
            if status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                next_task = state_obj.start_next_task()
                if next_task:
                    rich_ui.print_message(f"æ¬¡ã®ã‚¿ã‚¹ã‚¯é–‹å§‹: {next_task.description[:50]}...", "info")
                else:
                    rich_ui.print_success("ğŸ‰ å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
        except Exception as e:
            rich_ui.print_error(f"ã‚¿ã‚¹ã‚¯çŠ¶æ…‹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _update_pecking_order_progress(
        self, 
        state_obj: AgentState, 
        execution_result: Optional[ExecutionResult]
    ) -> None:
        """The Pecking Order ã®é€²æ—ã‚’æ›´æ–°ã™ã‚‹"""
        try:
            from ..state.pecking_order import TaskStatus
            
            current_task = state_obj.get_current_task()
            if not current_task:
                return
            
            # å®Ÿè¡Œçµæœã«åŸºã¥ã„ã¦ã‚¿ã‚¹ã‚¯çŠ¶æ…‹ã‚’æ±ºå®š
            if execution_result:
                if execution_result.success:
                    # æˆåŠŸæ™‚ã¯å®Œäº†çŠ¶æ…‹ã«æ›´æ–°
                    await self._update_current_task_status(
                        state_obj, 
                        TaskStatus.COMPLETED, 
                        result=execution_result.summary if hasattr(execution_result, 'summary') else "å®Ÿè¡Œå®Œäº†"
                    )
                else:
                    # å¤±æ•—æ™‚ã¯å¤±æ•—çŠ¶æ…‹ã«æ›´æ–°
                    await self._update_current_task_status(
                        state_obj, 
                        TaskStatus.FAILED, 
                        error=execution_result.error_message or "å®Ÿè¡Œå¤±æ•—"
                    )
            else:
                # å®Ÿè¡ŒçµæœãŒãªã„å ´åˆã¯é€²è¡Œä¸­çŠ¶æ…‹ã«æ›´æ–°
                await self._update_current_task_status(
                    state_obj, 
                    TaskStatus.IN_PROGRESS
                )
            
        except Exception as e:
            rich_ui.print_error(f"The Pecking Order é€²æ—æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_latest_user_message(self, state_obj: AgentState) -> Optional[str]:
        """æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹"""
        try:
            messages = state_obj.get_messages()
            for message in reversed(messages):
                if message.role == "user":
                    return message.content
            return None
        except Exception:
            return None


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯åˆæœŸåŒ–æ™‚ã«ä½œæˆ
five_node_orchestrator = None

def create_five_node_orchestrator(prompt_compiler, routing_engine):
    """5ãƒãƒ¼ãƒ‰ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    global five_node_orchestrator
    five_node_orchestrator = FiveNodeOrchestrator(prompt_compiler, routing_engine)
    return five_node_orchestrator