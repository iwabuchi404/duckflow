# Duckflow設定ファイル
# バージョン: 1.0

# LLMプロバイダー設定
llm:
  # メインプロバイダー (openai, anthropic, google, groq, openrouter)
  provider: "groq"
  
  # プロバイダー別設定
  openai:
    model: "gpt-4-turbo-preview"
    temperature: 0.1
    max_tokens: 4096
  
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    temperature: 0.1
    max_tokens: 4096
  
  google:
    model: "gemini-pro"
    temperature: 0.1
    max_tokens: 4096
  
  groq:
    model: "openai/gpt-oss-20b"
    temperature: 0.1
    max_tokens: 8192
  
  openrouter:
    model: "anthropic/claude-3-5-sonnet-20241022"
    temperature: 0.1
    max_tokens: 4096

# 要約用高速LLM設定 (ステップ2c)
summary_llm:
  # 要約専用プロバイダー (通常は高速・安価なモデルを使用)
  provider: "groq"
  
  # プロバイダー別設定
  openai:
    model: "gpt-4o-mini"  # 高速・安価
    temperature: 0.0
    max_tokens: 1000
  
  anthropic:
    model: "claude-3-haiku-20240307"  # 高速・安価
    temperature: 0.0
    max_tokens: 1000

# シンプル動的Duck Pacemaker設定
duck_pacemaker:
  # 動的制限機能（シンプル版）
  dynamic_limits:
    enabled: true
    min_loops: 3
    max_loops: 20
    
  # タスクプロファイル別ベース設定（改善版）
  task_profiles:
    simple_question:
      base_loops: 5
    code_analysis:
      base_loops: 12
    file_operation:
      base_loops: 8
    complex_reasoning:
      base_loops: 18
    multi_step_task:
      base_loops: 15
    general_chat:
      base_loops: 6
    creative_writing:
      base_loops: 10
    debugging:
      base_loops: 14
    research:
      base_loops: 16
      
  # バイタル閾値設定（シンプル版）
  vitals_thresholds:
    # D.U.C.K. Vitalsの正しい解釈:
    # mood = AIのプランに関する自信
    # focus = AIの思考の一貫性  
    # stamina = 1タスクの試行回数（消耗度）
    mood:
      low: 0.4
      good: 0.8
    focus:
      low: 0.4
      good: 0.8
    stamina:
      low: 0.3
      good: 0.8
    mood:
      critical: 0.4
      low: 0.6
      good: 0.8
      
  # ティア別制限
  tiers:
    conservative:
      min: 2
      max: 8
    balanced:
      min: 3
      max: 15
    aggressive:
      min: 5
      max: 25
  
  groq:
    model: "llama-3.1-8b-instant"  # 高速
    temperature: 0.0
    max_tokens: 1000
  
  openrouter:
    model: "meta-llama/llama-3.1-8b-instruct"  # 高速・安価
    temperature: 0.0
    max_tokens: 1000

# UI設定
ui:
  # UIタイプ (rich, textual)
  type: "rich"
  
  # Rich設定
  rich:
    theme: "monokai"
    highlight: true
    show_locals: true
  
  # Textual設定 (ステップ2で使用)
  textual:
    title: "Duckflow"
    dark_mode: true

# ツール設定
tools:
  # ファイル操作
  file_operations:
    max_file_size_mb: 10
    allowed_extensions: [".py", ".js", ".ts", ".jsx", ".tsx", ".html", ".css", ".json", ".yaml", ".yml", ".md", ".txt", ".csv"]
    backup_enabled: true
  
  # シェル実行 (ホワイトリスト)
  shell:
    allowed_commands:
      - "ls"
      - "dir"
      - "cat"
      - "type"
      - "grep"
      - "find"
      - "where"
      - "cd"
      - "pwd"
      - "echo"
      - "python"
      - "pip"
      - "uv" 
      - "pytest"
      - "black"
      - "ruff"
      - "mypy"
      - "git"
      - "node"
      - "npm"
    timeout_seconds: 30

# セキュリティ設定
security:
  # 人間による承認が必要な操作
  require_approval:
    file_write: true
    file_delete: true
    shell_execution: true
    directory_creation: true
  
  # 禁止パターン
  forbidden_patterns:
    - "rm -rf"
    - "sudo"
    - "chmod 777"
    - "passwd"

# ログ設定
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "logs/duckflow.log"
  max_size_mb: 50
  backup_count: 5
  
  # 対話ログ
  conversation:
    enabled: true
    file: "logs/conversation.json"

# Phase 1: Prompt/State/Transition/Safety
phase1:
  work_dir: "."               # リポジトリルート（相対→絶対に解決）
  safe_extensions: [".txt", ".md", ".py", ".json", ".yaml", ".yml", ".csv"]
  enable_transition_control: true
  max_transitions_per_utterance: 1
  enable_deep_plan_logging: true

# 記憶管理設定 (ステップ2c)
memory:
  # 短期記憶設定
  short_term:
    # 最大保持メッセージ数
    max_messages: 50
    # 自動クリーンアップ間隔(秒)
    cleanup_interval: 300
  
  # 中期記憶設定 (エピソード記憶)
  medium_term:
    # 要約トリガーのトークン数閾値
    summary_trigger_tokens: 4000
    # 要約の目標トークン数
    summary_target_tokens: 500
    # 要約後に保持する最新ターン数
    keep_recent_turns: 3
    # 要約の有効期限(時間)
    summary_expiry_hours: 24
  
  # 長期記憶設定 (将来拡張用)
  long_term:
    # 永続化ストレージ
    enabled: false
    # セッション間での記憶継続
    cross_session: false

# PromptSmith評価システム設定
promptsmith:
  # 評価モード設定
  evaluation:
    enabled: true
    separate_ai_roles: true  # 役割別AI使用の有効化
    timeout_seconds: 120     # 各シナリオのタイムアウト
    max_retries: 3          # 失敗時の再試行回数
  
  # TesterAI設定（挑戦的シナリオ生成）
  tester_ai:
    provider: "groq"
    model_settings:
      openai:
        model: "gpt-4o-mini"
        temperature: 0.3      # 創造的なシナリオ生成のため少し高め
        max_tokens: 2048
      anthropic:
        model: "claude-3-haiku-20240307"
        temperature: 0.3
        max_tokens: 2048
      groq:
        model: "llama-3.1-8b-instant"
        temperature: 0.3
        max_tokens: 2048
      openrouter:
        model: "meta-llama/llama-3.1-8b-instruct"
        temperature: 0.3
        max_tokens: 2048
  
  # EvaluatorAI設定（対話品質評価）
  evaluator_ai:
    provider: "anthropic"
    model_settings:
      openai:
        model: "gpt-4-turbo-preview"
        temperature: 0.0      # 一貫した評価のため低温度
        max_tokens: 4096
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        temperature: 0.0
        max_tokens: 4096
      groq:
        model: "openai/gpt-oss-20b"
        temperature: 0.0
        max_tokens: 4096
      openrouter:
        model: "anthropic/claude-3-5-sonnet-20241022"
        temperature: 0.0
        max_tokens: 4096
  
  # OptimizerAI設定（改善提案生成）
  optimizer_ai:
    provider: "openai"
    model_settings:
      openai:
        model: "gpt-4-turbo-preview"
        temperature: 0.2      # バランスの取れた提案のため
        max_tokens: 3072
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        temperature: 0.2
        max_tokens: 3072
      groq:
        model: "openai/gpt-oss-20b"
        temperature: 0.2
        max_tokens: 3072
      openrouter:
        model: "openai/gpt-4-turbo-preview"
        temperature: 0.2
        max_tokens: 3072
  
  # ConversationAnalyzer設定（対話分析）
  conversation_analyzer:
    provider: "anthropic"
    model_settings:
      openai:
        model: "gpt-4-turbo-preview"
        temperature: 0.1      # 正確な分析のため
        max_tokens: 2048
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        temperature: 0.1
        max_tokens: 2048
      groq:
        model: "openai/gpt-oss-20b"
        temperature: 0.1
        max_tokens: 2048
      openrouter:
        model: "anthropic/claude-3-5-sonnet-20241022"
        temperature: 0.1
        max_tokens: 2048
  
  # 被評価AI設定（実際のDuckflowエージェント）
  target_ai:
    use_main_llm: true      # メインLLM設定を使用するか
    override_provider: null # nullの場合はメインLLM、指定すればそのプロバイダーを使用
    override_model: null    # モデルの上書き指定

# The Duck Keeper - 統合アクセス制御システム設定
duck_keeper:
  # 許可される拡張子のホワイトリスト
  allowed_extensions:
    - ".py"
    - ".md"
    - ".json"
    - ".yaml"
    - ".yml"
    - ".txt"
    - ".csv"
    - ".js"
    - ".ts"
    - ".jsx"
    - ".tsx"
    - ".html"
    - ".css"
    - ".toml"
    - ".ini"
    - ".cfg"
    - ".log"
  
  # 常に除外するディレクトリのブラックリスト
  directory_blacklist:
    - ".git"
    - ".idea"
    - ".vscode"
    - "node_modules"
    - "__pycache__"
    - ".pytest_cache"
    - ".venv"
    - "venv"
    - ".env"
    - ".DS_Store"
    - "target"
    - "dist"
    - "build"
    - ".next"
    - ".nuxt"
    - "coverage"
  
  # プロジェクトワークスペース境界の強制
  enforce_workspace_boundary: true
  
  # .gitignoreファイルの尊重
  respect_gitignore: true
  
  # ファイル読み取り最大トークン数制限
  max_file_read_tokens: 8000
  
  # Duck Scanの高速検索設定
  scan_settings:
    # ripgrep使用の有効化
    use_ripgrep: true
    
    # 検索結果の最大数
    max_search_results: 100
    
    # ディレクトリスキャンの最大深度
    max_scan_depth: 10
    
    # キーワード検索のタイムアウト（秒）
    search_timeout: 30

# 開発設定
development:
  debug: false
  reload_on_change: false
  profiling: false

# 承認システム設定
approval:
  mode: "standard"           # strict | standard | trusted
  timeout_seconds: 30
  show_preview: true
  max_preview_length: 200
  ui:
    non_interactive: true     # 非対話UIを使用
    auto_approve_low: true    # 低リスクは自動承認
    auto_approve_high: false  # 高リスクの自動承認（必要時のみtrueに）
    auto_approve_all: false   # 全自動承認（非推奨）


# Phase 4 / LLM 呼び出し制御・観測設定
llm_max_retries: 3
llm_timeout_seconds: 30
gate_auto_stats: true
health_logging_enabled: true
health_logging_interval: 60


